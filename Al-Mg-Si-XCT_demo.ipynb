{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e868b5-d19f-4b3d-978a-a7ab22d83376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.2+cu118\n",
      "CUDA available: True\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Device name: Tesla P100-PCIE-12GB\n",
      "Loading dataset: Al-Mg-Si-XCT\n",
      "Image directory: ./Al-Mg-Si-XCT/images\n",
      "Mask directory: ./Al-Mg-Si-XCT/masks\n",
      "Found 150 valid image-mask pairs\n",
      "Loading dataset: AFM\n",
      "Image directory: ./AFM/images\n",
      "Mask directory: ./AFM/masks\n",
      "Found 46 valid image-mask pairs\n",
      "Loading dataset: Bond-Wire-XCT\n",
      "Image directory: ./Bond-Wire-XCT/images\n",
      "Mask directory: ./Bond-Wire-XCT/masks\n",
      "Found 0 valid image-mask pairs\n",
      "Loading dataset: Carbon\n",
      "Image directory: ./Carbon/images\n",
      "Mask directory: ./Carbon/masks\n",
      "Found 0 valid image-mask pairs\n",
      "Loading dataset: Fractography\n",
      "Image directory: ./Fractography/images\n",
      "Mask directory: ./Fractography/masks\n",
      "Found 0 valid image-mask pairs\n",
      "\n",
      "Dataset Summary:\n",
      "Al-Mg-Si-XCT: 150 image-mask pairs\n",
      "AFM: 46 image-mask pairs\n",
      "Bond-Wire-XCT: 0 image-mask pairs\n",
      "Carbon: 0 image-mask pairs\n",
      "Fractography: 0 image-mask pairs\n",
      "Testing MicroNet model loading...\n",
      "Successfully loaded MicroNet weights for micronet-resnet50 from resnet50_micronet_weights.pth\n",
      "Successfully loaded MicroNet weights for micronet-se_resnext101 from resnext101_micronet_weights.pth\n",
      "✓ MicroNet ResNet50 weights loaded successfully\n",
      "  Keys in state dict: 318\n",
      "✓ MicroNet SE-ResNeXt101 weights loaded successfully\n",
      "  Keys in state dict: 624\n",
      "GPU Memory available: 11.9 GB\n",
      "Starting Al-Mg-Si-XCT data inspection...\n",
      "AL-MG-SI-XCT DATA INSPECTION\n",
      "----------------------------------------\n",
      "Loading dataset: Al-Mg-Si-XCT\n",
      "Image directory: ./Al-Mg-Si-XCT/images\n",
      "Mask directory: ./Al-Mg-Si-XCT/masks\n",
      "Found 150 valid image-mask pairs\n",
      "Mask statistics:\n",
      "  Total pixels: 6300100\n",
      "  Positive pixels: 2720 (0.04%)\n",
      "  Negative pixels: 6297380.0 (99.96%)\n",
      "  Imbalance ratio: 2315.2:1\n",
      "Found 150 Al-Mg-Si-XCT image-mask pairs\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Al-Mg-Si-XCT experiment...\n",
      "MDSMG: Material Data Science Model Garden\n",
      "Al-Mg-Si-XCT Dataset Experiment\n",
      "================================================================================\n",
      "Starting Al-Mg-Si-XCT experiment...\n",
      "Starting MDSMG Al-Mg-Si-XCT Experiment on cuda\n",
      "Total experiments to run: 12\n",
      "Running experiment 1/12: resnet50_unet_imagenet\n",
      "EXPERIMENT: Al-Mg-Si-XCT | resnet50 | unet | imagenet\n",
      "================================================================================\n",
      "Loading dataset: Al-Mg-Si-XCT\n",
      "Image directory: ./Al-Mg-Si-XCT/images\n",
      "Mask directory: ./Al-Mg-Si-XCT/masks\n",
      "Found 150 valid image-mask pairs\n",
      "Dataset split: 120 train, 30 validation\n",
      "Creating encoder: resnet50 with imagenet pretraining\n",
      "✓ ResNet50 with ImageNet weights loaded\n",
      "Training resnet50_unet_imagenet for 200 epochs on cuda\n",
      "Using Focal Loss for class imbalance\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 100%|██████████| 60/60 [00:49<00:00,  1.21it/s, train_loss=0.0197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1: Train Loss: 0.0324 | Val Loss: 0.0208 | Val IoU: 0.0000 | Val Dice: 0.0000\n",
      "  New best IoU: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/200: 100%|██████████| 60/60 [00:40<00:00,  1.49it/s, train_loss=0.0107]\n",
      "Epoch 3/200:  33%|███▎      | 20/60 [00:14<00:28,  1.42it/s, train_loss=0.0103]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 938\u001b[0m\n\u001b[1;32m    935\u001b[0m inspect_almgsi_data()\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting Al-Mg-Si-XCT experiment...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 938\u001b[0m almgsi_results, almgsi_histories \u001b[38;5;241m=\u001b[39m \u001b[43mmain_almgsi_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 816\u001b[0m, in \u001b[0;36mmain_almgsi_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Al-Mg-Si-XCT experiment...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 816\u001b[0m almgsi_results, almgsi_histories \u001b[38;5;241m=\u001b[39m \u001b[43mrun_almgsi_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(almgsi_results) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnalyzing results...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 712\u001b[0m, in \u001b[0;36mrun_almgsi_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m    708\u001b[0m experiment_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning experiment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_experiments\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecoder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 712\u001b[0m result, history \u001b[38;5;241m=\u001b[39m \u001b[43mrun_single_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m history:\n\u001b[1;32m    717\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[0;32mIn[1], line 668\u001b[0m, in \u001b[0;36mrun_single_experiment\u001b[0;34m(dataset_name, encoder_name, decoder_name, pretrained, device)\u001b[0m\n\u001b[1;32m    664\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    666\u001b[0m model \u001b[38;5;241m=\u001b[39m MaterialSegmentationModel(encoder_name, decoder_name, pretrained, n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 668\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m result \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset_name,\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m'\u001b[39m: encoder_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_val_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_losses\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    682\u001b[0m }\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRESULTS: IoU=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_val_iou\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Dice=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_val_dice\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 560\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, device)\u001b[0m\n\u001b[1;32m    557\u001b[0m train_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    559\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 560\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m    561\u001b[0m     images, masks \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), masks\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    563\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[1], line 61\u001b[0m, in \u001b[0;36mMaterialDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Handle different bit depths (uint16, uint8, etc.)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39muint16:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Use percentile-based normalization for better contrast\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     p1, p99 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpercentile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m99\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip((image \u001b[38;5;241m-\u001b[39m p1) \u001b[38;5;241m/\u001b[39m (p99 \u001b[38;5;241m-\u001b[39m p1) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39muint8:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:4283\u001b[0m, in \u001b[0;36mpercentile\u001b[0;34m(a, q, axis, out, overwrite_input, method, keepdims, interpolation)\u001b[0m\n\u001b[1;32m   4281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _quantile_is_valid(q):\n\u001b[1;32m   4282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPercentiles must be in the range [0, 100]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 4283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_quantile_unchecked\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4284\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:4555\u001b[0m, in \u001b[0;36m_quantile_unchecked\u001b[0;34m(a, q, axis, out, overwrite_input, method, keepdims)\u001b[0m\n\u001b[1;32m   4547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_quantile_unchecked\u001b[39m(a,\n\u001b[1;32m   4548\u001b[0m                         q,\n\u001b[1;32m   4549\u001b[0m                         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4552\u001b[0m                         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   4553\u001b[0m                         keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   4554\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Assumes that q is in [0, 1], and is an ndarray\"\"\"\u001b[39;00m\n\u001b[0;32m-> 4555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ureduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4556\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_quantile_ureduce_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4557\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4558\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4559\u001b[0m \u001b[43m                    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4560\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4561\u001b[0m \u001b[43m                    \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4562\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:3823\u001b[0m, in \u001b[0;36m_ureduce\u001b[0;34m(a, func, keepdims, **kwargs)\u001b[0m\n\u001b[1;32m   3820\u001b[0m             index_out \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, ) \u001b[38;5;241m*\u001b[39m nd\n\u001b[1;32m   3821\u001b[0m             kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out[(\u001b[38;5;28mEllipsis\u001b[39m, ) \u001b[38;5;241m+\u001b[39m index_out]\n\u001b[0;32m-> 3823\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3825\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:4722\u001b[0m, in \u001b[0;36m_quantile_ureduce_func\u001b[0;34m(a, q, axis, out, overwrite_input, method)\u001b[0m\n\u001b[1;32m   4720\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4721\u001b[0m         arr \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m-> 4722\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_quantile\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4723\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mquantiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4724\u001b[0m \u001b[43m                   \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4725\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4726\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4727\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:4824\u001b[0m, in \u001b[0;36m_quantile\u001b[0;34m(arr, quantiles, axis, method, out)\u001b[0m\n\u001b[1;32m   4820\u001b[0m previous_indexes, next_indexes \u001b[38;5;241m=\u001b[39m _get_indexes(arr,\n\u001b[1;32m   4821\u001b[0m                                               virtual_indexes,\n\u001b[1;32m   4822\u001b[0m                                               values_count)\n\u001b[1;32m   4823\u001b[0m \u001b[38;5;66;03m# --- Sorting\u001b[39;00m\n\u001b[0;32m-> 4824\u001b[0m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartition\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4826\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mprevious_indexes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4827\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mnext_indexes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4828\u001b[0m \u001b[43m                              \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4829\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m supports_nans:\n\u001b[1;32m   4831\u001b[0m     slices_having_nans \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(arr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Material Data Science Model Garden (MDSMG) Experiment\n",
    "# Systematic Evaluation of Encoder-Decoder Architectures for Material Image Segmentation\n",
    "# Al-Mg-Si-XCT Dataset Experiment\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "import segmentation_models_pytorch as smp\n",
    "from huggingface_hub import hf_hub_download\n",
    "import timm\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current device:\", torch.cuda.current_device())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name())\n",
    "\n",
    "class MaterialDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transform=None, image_size=(256, 256)):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform\n",
    "        self.image_size = image_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "        \n",
    "        # Load image with proper handling for different bit depths\n",
    "        if image_path.endswith('.tif') or image_path.endswith('.tiff'):\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "            \n",
    "            if len(image.shape) == 3:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            elif len(image.shape) == 2:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "            \n",
    "            # Handle different bit depths (uint16, uint8, etc.)\n",
    "            if image.dtype == np.uint16:\n",
    "                # Use percentile-based normalization for better contrast\n",
    "                p1, p99 = np.percentile(image, (1, 99))\n",
    "                image = np.clip((image - p1) / (p99 - p1) * 255, 0, 255).astype(np.uint8)\n",
    "            elif image.dtype != np.uint8:\n",
    "                image = image.astype(np.uint8)\n",
    "                \n",
    "        else:\n",
    "            image = np.array(Image.open(image_path).convert('RGB'))\n",
    "            \n",
    "        # Load mask with proper handling\n",
    "        if mask_path.endswith('.tif') or mask_path.endswith('.tiff'):\n",
    "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            # Handle different mask bit depths\n",
    "            if mask.dtype == np.uint16:\n",
    "                mask_max = mask.max()\n",
    "                if mask_max > 255:\n",
    "                    mask = (mask.astype(np.float32) / mask_max * 255.0).astype(np.uint8)\n",
    "                else:\n",
    "                    mask = mask.astype(np.uint8)\n",
    "            elif mask.dtype != np.uint8:\n",
    "                mask = mask.astype(np.uint8)\n",
    "        else:\n",
    "            mask = np.array(Image.open(mask_path).convert('L'))\n",
    "        \n",
    "        # Resize images\n",
    "        image = cv2.resize(image, self.image_size)\n",
    "        mask = cv2.resize(mask, self.image_size, interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        # More careful mask binarization\n",
    "        unique_vals = np.unique(mask)\n",
    "        \n",
    "        # Create binary mask\n",
    "        if len(unique_vals) == 2 and 0 in unique_vals:\n",
    "            mask = (mask > 0).astype(np.uint8)\n",
    "        else:\n",
    "            threshold = np.percentile(mask[mask > 0], 50) if np.any(mask > 0) else 127\n",
    "            mask = (mask > threshold).astype(np.uint8)\n",
    "        \n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "        else:\n",
    "            # Convert to float32 before creating tensor\n",
    "            image = image.astype(np.float32)\n",
    "            image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n",
    "            mask = torch.from_numpy(mask.astype(np.float32)).float()\n",
    "            \n",
    "        return image, mask\n",
    "\n",
    "def load_dataset_paths(base_path, dataset_name):\n",
    "    dataset_path = os.path.join(base_path, dataset_name)\n",
    "    image_dir = os.path.join(dataset_path, 'images')\n",
    "    mask_dir = os.path.join(dataset_path, 'masks')\n",
    "    \n",
    "    print(f\"Loading dataset: {dataset_name}\")\n",
    "    print(f\"Image directory: {image_dir}\")\n",
    "    print(f\"Mask directory: {mask_dir}\")\n",
    "    \n",
    "    if not os.path.exists(image_dir) or not os.path.exists(mask_dir):\n",
    "        print(f\"Warning: Directory does not exist for {dataset_name}\")\n",
    "        return [], []\n",
    "    \n",
    "    image_extensions = ['*.png', '*.tif', '*.tiff', '*.jpg', '*.jpeg']\n",
    "    image_paths = []\n",
    "    for ext in image_extensions:\n",
    "        image_paths.extend(glob.glob(os.path.join(image_dir, ext)))\n",
    "    \n",
    "    mask_paths = []\n",
    "    for img_path in image_paths:\n",
    "        img_name = os.path.basename(img_path)\n",
    "        img_name_no_ext = os.path.splitext(img_name)[0]\n",
    "        \n",
    "        for mask_ext in ['png', 'tif', 'tiff']:\n",
    "            mask_path = os.path.join(mask_dir, f\"{img_name_no_ext}.{mask_ext}\")\n",
    "            if os.path.exists(mask_path):\n",
    "                mask_paths.append(mask_path)\n",
    "                break\n",
    "    \n",
    "    valid_pairs = []\n",
    "    valid_mask_paths = []\n",
    "    for img_path, mask_path in zip(image_paths, mask_paths):\n",
    "        if os.path.exists(img_path) and os.path.exists(mask_path):\n",
    "            valid_pairs.append(img_path)\n",
    "            valid_mask_paths.append(mask_path)\n",
    "    \n",
    "    print(f\"Found {len(valid_pairs)} valid image-mask pairs\")\n",
    "    return valid_pairs, valid_mask_paths\n",
    "\n",
    "base_path = \".\"\n",
    "datasets = ['Al-Mg-Si-XCT', 'AFM', 'Bond-Wire-XCT', 'Carbon', 'Fractography']\n",
    "\n",
    "dataset_info = {}\n",
    "for dataset_name in datasets:\n",
    "    image_paths, mask_paths = load_dataset_paths(base_path, dataset_name)\n",
    "    dataset_info[dataset_name] = len(image_paths)\n",
    "    \n",
    "print(\"\\nDataset Summary:\")\n",
    "for name, count in dataset_info.items():\n",
    "    print(f\"{name}: {count} image-mask pairs\")\n",
    "\n",
    "def load_micronet_weights(model_name='micronet-resnet50'):\n",
    "    try:\n",
    "        if model_name == 'micronet-resnet50':\n",
    "            filenames_to_try = [\n",
    "                \"resnet50_micronet_weights.pth\",\n",
    "                \"resnet50_imagenet-micronet_weights.pth\", \n",
    "                \"pytorch_model.bin\"\n",
    "            ]\n",
    "            repo_id = \"jstuckner/microscopy-resnet50-micronet\"\n",
    "            \n",
    "        elif model_name == 'micronet-se_resnext101':\n",
    "            filenames_to_try = [\n",
    "                \"resnext101_micronet_weights.pth\",\n",
    "                \"resnext101_imagenet-micronet_weights.pth\",\n",
    "                \"pytorch_model.bin\"\n",
    "            ]\n",
    "            repo_id = \"jstuckner/microscopy-resnext101-micronet\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown MicroNet model: {model_name}\")\n",
    "        \n",
    "        weights = None\n",
    "        for filename in filenames_to_try:\n",
    "            try:\n",
    "                model_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "                weights = torch.load(model_path, map_location='cpu')\n",
    "                print(f\"Successfully loaded MicroNet weights for {model_name} from {filename}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {filename}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if weights is None:\n",
    "            print(f\"Failed to load any weights for {model_name}\")\n",
    "            \n",
    "        return weights\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load MicroNet weights for {model_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"Testing MicroNet model loading...\")\n",
    "micronet_resnet50_weights = load_micronet_weights('micronet-resnet50')\n",
    "micronet_se_resnext101_weights = load_micronet_weights('micronet-se_resnext101')\n",
    "\n",
    "if micronet_resnet50_weights:\n",
    "    print(\"✓ MicroNet ResNet50 weights loaded successfully\")\n",
    "    print(f\"  Keys in state dict: {len(micronet_resnet50_weights.keys())}\")\n",
    "    \n",
    "if micronet_se_resnext101_weights:\n",
    "    print(\"✓ MicroNet SE-ResNeXt101 weights loaded successfully\")\n",
    "    print(f\"  Keys in state dict: {len(micronet_se_resnext101_weights.keys())}\")\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.squeeze(x).view(b, c)\n",
    "        y = self.excitation(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "def create_encoder(encoder_name='resnet50', pretrained='imagenet'):\n",
    "    print(f\"Creating encoder: {encoder_name} with {pretrained} pretraining\")\n",
    "    \n",
    "    if encoder_name == 'resnet50':\n",
    "        if pretrained == 'imagenet':\n",
    "            encoder = smp.encoders.get_encoder('resnet50', in_channels=3, weights='imagenet')\n",
    "            print(\"✓ ResNet50 with ImageNet weights loaded\")\n",
    "        elif pretrained == 'micronet':\n",
    "            encoder = smp.encoders.get_encoder('resnet50', in_channels=3, weights=None)\n",
    "            micronet_weights = load_micronet_weights('micronet-resnet50')\n",
    "            if micronet_weights:\n",
    "                try:\n",
    "                    encoder.load_state_dict(micronet_weights, strict=False)\n",
    "                    print(\"✓ ResNet50 with MicroNet weights loaded\")\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠ Failed to load MicroNet weights strictly, trying partial loading: {str(e)}\")\n",
    "                    model_dict = encoder.state_dict()\n",
    "                    pretrained_dict = {k: v for k, v in micronet_weights.items() \n",
    "                                     if k in model_dict and v.shape == model_dict[k].shape}\n",
    "                    model_dict.update(pretrained_dict)\n",
    "                    encoder.load_state_dict(model_dict)\n",
    "                    print(f\"✓ ResNet50 with partial MicroNet weights loaded ({len(pretrained_dict)} layers)\")\n",
    "            else:\n",
    "                print(\"⚠ Failed to load MicroNet weights, using random initialization\")\n",
    "        else:\n",
    "            encoder = smp.encoders.get_encoder('resnet50', in_channels=3, weights=None)\n",
    "            print(\"✓ ResNet50 with random weights\")\n",
    "            \n",
    "        encoder_channels = [3, 64, 256, 512, 1024, 2048]\n",
    "    \n",
    "    elif encoder_name == 'se_resnext101':\n",
    "        if pretrained == 'imagenet':\n",
    "            encoder = smp.encoders.get_encoder('se_resnext101_32x4d', in_channels=3, weights='imagenet')\n",
    "            print(\"✓ SE-ResNeXt101 with ImageNet weights loaded\")\n",
    "        elif pretrained == 'micronet':\n",
    "            print(\"⚠ MicroNet weights for SE-ResNeXt101 have architecture mismatch\")\n",
    "            print(\"  Using ImageNet pretrained weights as fallback\")\n",
    "            encoder = smp.encoders.get_encoder('se_resnext101_32x4d', in_channels=3, weights='imagenet')\n",
    "            \n",
    "            micronet_weights = load_micronet_weights('micronet-se_resnext101')\n",
    "            if micronet_weights:\n",
    "                try:\n",
    "                    model_dict = encoder.state_dict()\n",
    "                    pretrained_dict = {}\n",
    "                    for k, v in micronet_weights.items():\n",
    "                        if k in model_dict and v.shape == model_dict[k].shape:\n",
    "                            pretrained_dict[k] = v\n",
    "                    \n",
    "                    if len(pretrained_dict) > 0:\n",
    "                        model_dict.update(pretrained_dict)\n",
    "                        encoder.load_state_dict(model_dict)\n",
    "                        print(f\"✓ Loaded {len(pretrained_dict)} compatible layers from MicroNet\")\n",
    "                    else:\n",
    "                        print(\"✗ No compatible layers found in MicroNet weights\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"✗ Failed to load any MicroNet weights: {str(e)}\")\n",
    "            \n",
    "        else:\n",
    "            encoder = smp.encoders.get_encoder('se_resnext101_32x4d', in_channels=3, weights=None)\n",
    "            print(\"✓ SE-ResNeXt101 with random weights\")\n",
    "            \n",
    "        encoder_channels = [3, 64, 256, 512, 1024, 2048]\n",
    "    \n",
    "    return encoder, encoder_channels\n",
    "\n",
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, encoder_channels, decoder_channels, n_classes=1):\n",
    "        super(UNetDecoder, self).__init__()\n",
    "        \n",
    "        self.decoder_blocks = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(decoder_channels)):\n",
    "            if i == 0:\n",
    "                in_ch = encoder_channels[-1]\n",
    "            else:\n",
    "                in_ch = decoder_channels[i-1]\n",
    "            \n",
    "            out_ch = decoder_channels[i]\n",
    "            skip_ch = encoder_channels[-(i+2)] if i < len(encoder_channels)-1 else 0\n",
    "            \n",
    "            block = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2),\n",
    "                nn.Conv2d(out_ch + skip_ch, out_ch, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "            \n",
    "            self.decoder_blocks.append(block)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(decoder_channels[-1], n_classes, kernel_size=1)\n",
    "        \n",
    "    def forward(self, encoder_features):\n",
    "        x = encoder_features[-1]\n",
    "        \n",
    "        for i, decoder_block in enumerate(self.decoder_blocks):\n",
    "            x = decoder_block[0](x)\n",
    "            \n",
    "            if i < len(encoder_features) - 1:\n",
    "                skip = encoder_features[-(i+2)]\n",
    "                if x.shape[2:] != skip.shape[2:]:\n",
    "                    x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
    "                x = torch.cat([x, skip], dim=1)\n",
    "            \n",
    "            for layer in decoder_block[1:]:\n",
    "                x = layer(x)\n",
    "        \n",
    "        return self.final_conv(x)\n",
    "\n",
    "class ASPPModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, atrous_rates):\n",
    "        super(ASPPModule, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.atrous_convs = nn.ModuleList()\n",
    "        for rate in atrous_rates:\n",
    "            self.atrous_convs.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, padding=rate, dilation=rate, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ))\n",
    "        \n",
    "        self.global_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(out_channels * (2 + len(atrous_rates)), out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[2:]\n",
    "        \n",
    "        feat1 = self.conv1(x)\n",
    "        atrous_feats = [conv(x) for conv in self.atrous_convs]\n",
    "        global_feat = self.global_pool(x)\n",
    "        global_feat = F.interpolate(global_feat, size=(h, w), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        concat_feat = torch.cat([feat1] + atrous_feats + [global_feat], dim=1)\n",
    "        return self.project(concat_feat)\n",
    "\n",
    "class DeepLabV3Decoder(nn.Module):\n",
    "    def __init__(self, encoder_channels, n_classes=1):\n",
    "        super(DeepLabV3Decoder, self).__init__()\n",
    "        self.aspp = ASPPModule(encoder_channels[-1], 256, [6, 12, 18])\n",
    "        self.classifier = nn.Conv2d(256, n_classes, 1)\n",
    "        \n",
    "    def forward(self, encoder_features):\n",
    "        x = encoder_features[-1]\n",
    "        x = self.aspp(x)\n",
    "        x = self.classifier(x)\n",
    "        x = F.interpolate(x, scale_factor=32, mode='bilinear', align_corners=False)\n",
    "        return x\n",
    "\n",
    "class DeepLabV3PlusDecoder(nn.Module):\n",
    "    def __init__(self, encoder_channels, n_classes=1):\n",
    "        super(DeepLabV3PlusDecoder, self).__init__()\n",
    "        \n",
    "        self.aspp = ASPPModule(encoder_channels[-1], 256, [6, 12, 18])\n",
    "        low_level_channels = encoder_channels[1]\n",
    "        \n",
    "        self.low_level_proj = nn.Sequential(\n",
    "            nn.Conv2d(low_level_channels, 48, 1, bias=False),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(256 + 48, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Conv2d(256, n_classes, 1)\n",
    "        \n",
    "    def forward(self, encoder_features):\n",
    "        low_level = encoder_features[1]\n",
    "        high_level = encoder_features[-1]\n",
    "        \n",
    "        high_level = self.aspp(high_level)\n",
    "        high_level = F.interpolate(high_level, size=low_level.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        low_level = self.low_level_proj(low_level)\n",
    "        \n",
    "        concat_feat = torch.cat([high_level, low_level], dim=1)\n",
    "        decoded = self.decoder(concat_feat)\n",
    "        output = self.classifier(decoded)\n",
    "        \n",
    "        output = F.interpolate(output, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class MaterialSegmentationModel(nn.Module):\n",
    "    def __init__(self, encoder_name='resnet50', decoder_name='unet', pretrained='imagenet', n_classes=1):\n",
    "        super(MaterialSegmentationModel, self).__init__()\n",
    "        \n",
    "        self.encoder, encoder_channels = create_encoder(encoder_name, pretrained)\n",
    "        \n",
    "        if decoder_name == 'unet':\n",
    "            self.decoder = UNetDecoder(encoder_channels[1:], [256, 128, 64, 32], n_classes)\n",
    "        elif decoder_name == 'deeplabv3':\n",
    "            self.decoder = DeepLabV3Decoder(encoder_channels[1:], n_classes)\n",
    "        elif decoder_name == 'deeplabv3plus':\n",
    "            self.decoder = DeepLabV3PlusDecoder(encoder_channels, n_classes)\n",
    "        \n",
    "        self.config = f\"{encoder_name}_{decoder_name}_{pretrained}\"\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoder_features = self.encoder(x)\n",
    "        \n",
    "        if hasattr(self.decoder, '__class__') and self.decoder.__class__.__name__ == 'DeepLabV3PlusDecoder':\n",
    "            output = self.decoder(encoder_features)\n",
    "        else:\n",
    "            output = self.decoder(encoder_features[1:])\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Weighted BCE Loss for class imbalance\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, pos_weight=10.0):\n",
    "        super(WeightedBCELoss, self).__init__()\n",
    "        self.pos_weight = pos_weight\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        # Calculate dynamic positive weight based on class distribution\n",
    "        pos_pixels = targets.sum()\n",
    "        neg_pixels = (targets.numel() - pos_pixels)\n",
    "        \n",
    "        if pos_pixels > 0:\n",
    "            dynamic_pos_weight = (neg_pixels / pos_pixels).clamp(min=1.0, max=50.0)\n",
    "        else:\n",
    "            dynamic_pos_weight = torch.tensor(self.pos_weight)\n",
    "            \n",
    "        return F.binary_cross_entropy_with_logits(inputs, targets, \n",
    "                                                pos_weight=dynamic_pos_weight)\n",
    "\n",
    "# Focal Loss for extreme class imbalance\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "def dice_coefficient(pred, target, smooth=1e-6):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred = (pred > 0.5).float()\n",
    "    \n",
    "    # Ensure both tensors have same shape\n",
    "    if target.dim() == 3 and pred.dim() == 4:\n",
    "        target = target.unsqueeze(1)\n",
    "    elif target.dim() == 4 and pred.dim() == 3:\n",
    "        pred = pred.unsqueeze(1)\n",
    "    \n",
    "    intersection = (pred * target).sum(dim=(2, 3))\n",
    "    union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
    "    \n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return dice.mean()\n",
    "\n",
    "def iou_score(pred, target, smooth=1e-6):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred = (pred > 0.5).float()\n",
    "    \n",
    "    # Ensure both tensors have same shape\n",
    "    if target.dim() == 3 and pred.dim() == 4:\n",
    "        target = target.unsqueeze(1)\n",
    "    elif target.dim() == 4 and pred.dim() == 3:\n",
    "        pred = pred.unsqueeze(1)\n",
    "    \n",
    "    intersection = (pred * target).sum(dim=(2, 3))\n",
    "    union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3)) - intersection\n",
    "    \n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return iou.mean()\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=100, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    \n",
    "    # Use Focal Loss for extreme class imbalance\n",
    "    criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5, verbose=False)\n",
    "    \n",
    "    best_val_iou = 0\n",
    "    history = {\n",
    "        'train_losses': [],\n",
    "        'val_losses': [],\n",
    "        'val_ious': [],\n",
    "        'val_dices': []\n",
    "    }\n",
    "    \n",
    "    print(f\"Training {model.config} for {num_epochs} epochs on {device}\")\n",
    "    print(f\"Using Focal Loss for class imbalance\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for images, masks in pbar:\n",
    "            images, masks = images.to(device), masks.to(device).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            if outputs.shape[2:] != masks.shape[1:]:\n",
    "                outputs = F.interpolate(outputs, size=masks.shape[1:], mode='bilinear', align_corners=False)\n",
    "            \n",
    "            outputs = outputs.squeeze(1)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "            \n",
    "            pbar.set_postfix({'train_loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            if train_batches % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_iou = 0\n",
    "        val_dice = 0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(device), masks.to(device).float()\n",
    "                \n",
    "                outputs = model(images)\n",
    "                if outputs.shape[2:] != masks.shape[1:]:\n",
    "                    outputs = F.interpolate(outputs, size=masks.shape[1:], mode='bilinear', align_corners=False)\n",
    "                \n",
    "                outputs = outputs.squeeze(1)\n",
    "                loss = criterion(outputs, masks)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Ensure consistent dimensions for metrics\n",
    "                if masks.dim() == 3:\n",
    "                    masks_for_metrics = masks.unsqueeze(1)\n",
    "                else:\n",
    "                    masks_for_metrics = masks\n",
    "                \n",
    "                if outputs.dim() == 3:\n",
    "                    outputs_for_metrics = outputs.unsqueeze(1)\n",
    "                else:\n",
    "                    outputs_for_metrics = outputs\n",
    "                \n",
    "                val_iou += iou_score(outputs_for_metrics, masks_for_metrics).item()\n",
    "                val_dice += dice_coefficient(outputs_for_metrics, masks_for_metrics).item()\n",
    "                val_batches += 1\n",
    "        \n",
    "        avg_train_loss = train_loss / train_batches\n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        avg_val_iou = val_iou / val_batches\n",
    "        avg_val_dice = val_dice / val_batches\n",
    "        \n",
    "        history['train_losses'].append(avg_train_loss)\n",
    "        history['val_losses'].append(avg_val_loss)\n",
    "        history['val_ious'].append(avg_val_iou)\n",
    "        history['val_dices'].append(avg_val_dice)\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        if epoch % 10 == 0 or epoch >= num_epochs - 5:\n",
    "            print(f'Epoch {epoch+1:3d}: Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val IoU: {avg_val_iou:.4f} | Val Dice: {avg_val_dice:.4f}')\n",
    "        \n",
    "        if avg_val_iou > best_val_iou:\n",
    "            best_val_iou = avg_val_iou\n",
    "            torch.save(model.state_dict(), f'best_{model.config}.pth')\n",
    "            if epoch % 10 == 0 or epoch >= num_epochs - 5:\n",
    "                print(f'  New best IoU: {best_val_iou:.4f}')\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    history['best_val_iou'] = best_val_iou\n",
    "    print(f\"Training completed! Best validation IoU: {best_val_iou:.4f}\")\n",
    "    return history\n",
    "\n",
    "def run_single_experiment(dataset_name, encoder_name, decoder_name, pretrained, device='cuda'):\n",
    "    print(f\"EXPERIMENT: {dataset_name} | {encoder_name} | {decoder_name} | {pretrained}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    image_paths, mask_paths = load_dataset_paths(\".\", dataset_name)\n",
    "    \n",
    "    if len(image_paths) < 4:\n",
    "        print(f\"Insufficient data for {dataset_name}: {len(image_paths)} samples\")\n",
    "        return None, None\n",
    "    \n",
    "    train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        image_paths, mask_paths, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset split: {len(train_images)} train, {len(val_images)} validation\")\n",
    "    \n",
    "    train_dataset = MaterialDataset(train_images, train_masks, image_size=(256, 256))\n",
    "    val_dataset = MaterialDataset(val_images, val_masks, image_size=(256, 256))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=0)\n",
    "    \n",
    "    model = MaterialSegmentationModel(encoder_name, decoder_name, pretrained, n_classes=1)\n",
    "    \n",
    "    history = train_model(model, train_loader, val_loader, num_epochs=200, device=device)\n",
    "    \n",
    "    result = {\n",
    "        'dataset': dataset_name,\n",
    "        'encoder': encoder_name,\n",
    "        'decoder': decoder_name,\n",
    "        'pretrained': pretrained,\n",
    "        'config': f\"{encoder_name}_{decoder_name}_{pretrained}\",\n",
    "        'train_samples': len(train_images),\n",
    "        'val_samples': len(val_images),\n",
    "        'best_val_iou': history['best_val_iou'],\n",
    "        'final_val_dice': history['val_dices'][-1],\n",
    "        'final_train_loss': history['train_losses'][-1],\n",
    "        'final_val_loss': history['val_losses'][-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"RESULTS: IoU={result['best_val_iou']:.4f} | Dice={result['final_val_dice']:.4f}\")\n",
    "    \n",
    "    return result, history\n",
    "\n",
    "def run_almgsi_experiment():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Starting MDSMG Al-Mg-Si-XCT Experiment on {device}\")\n",
    "    \n",
    "    datasets = ['Al-Mg-Si-XCT']\n",
    "    encoders = ['resnet50', 'se_resnext101']\n",
    "    decoders = ['unet', 'deeplabv3', 'deeplabv3plus']\n",
    "    pretrained_options = ['imagenet', 'micronet']\n",
    "    \n",
    "    results = []\n",
    "    histories = {}\n",
    "    experiment_count = 0\n",
    "    \n",
    "    total_experiments = len(datasets) * len(encoders) * len(decoders) * len(pretrained_options)\n",
    "    print(f\"Total experiments to run: {total_experiments}\")\n",
    "    \n",
    "    for dataset_name in datasets:\n",
    "        for encoder in encoders:\n",
    "            for decoder in decoders:\n",
    "                for pretrained in pretrained_options:\n",
    "                    experiment_count += 1\n",
    "                    \n",
    "                    print(f\"Running experiment {experiment_count}/{total_experiments}: {encoder}_{decoder}_{pretrained}\")\n",
    "                    \n",
    "                    result, history = run_single_experiment(\n",
    "                        dataset_name, encoder, decoder, pretrained, device\n",
    "                    )\n",
    "                    \n",
    "                    if result and history:\n",
    "                        results.append(result)\n",
    "                        histories[result['config'] + '_' + dataset_name] = history\n",
    "                        print(f\"Experiment {experiment_count}/{total_experiments} completed: {result['config']}\")\n",
    "                    else:\n",
    "                        print(f\"Experiment {experiment_count}/{total_experiments} failed or skipped\")\n",
    "    \n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv('almgsi_experiment_results.csv', index=False)\n",
    "        \n",
    "        print(f\"Al-Mg-Si-XCT experiment suite completed!\")\n",
    "        print(f\"Total successful experiments: {len(results)}\")\n",
    "        print(f\"Results saved to: almgsi_experiment_results.csv\")\n",
    "        \n",
    "        return results_df, histories\n",
    "    else:\n",
    "        print(\"No successful experiments completed\")\n",
    "        return pd.DataFrame(), {}\n",
    "\n",
    "def analyze_almgsi_results(results_df):\n",
    "    print(\"AL-MG-SI-XCT EXPERIMENT RESULTS ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Total experiments: {len(results_df)}\")\n",
    "    print(f\"Mean IoU across all experiments: {results_df['best_val_iou'].mean():.4f}\")\n",
    "    print(f\"Best overall IoU: {results_df['best_val_iou'].max():.4f}\")\n",
    "    \n",
    "    print(\"\\nPERFORMANCE BY ENCODER:\")\n",
    "    encoder_performance = results_df.groupby('encoder')['best_val_iou'].agg(['mean', 'std', 'max', 'count'])\n",
    "    encoder_performance.columns = ['Mean_IoU', 'Std_IoU', 'Max_IoU', 'Count']\n",
    "    print(encoder_performance.round(4))\n",
    "    \n",
    "    print(\"\\nPERFORMANCE BY DECODER:\")\n",
    "    decoder_performance = results_df.groupby('decoder')['best_val_iou'].agg(['mean', 'std', 'max', 'count'])\n",
    "    decoder_performance.columns = ['Mean_IoU', 'Std_IoU', 'Max_IoU', 'Count']\n",
    "    print(decoder_performance.round(4))\n",
    "    \n",
    "    print(\"\\nPERFORMANCE BY PRETRAINING:\")\n",
    "    pretrained_performance = results_df.groupby('pretrained')['best_val_iou'].agg(['mean', 'std', 'max', 'count'])\n",
    "    pretrained_performance.columns = ['Mean_IoU', 'Std_IoU', 'Max_IoU', 'Count']\n",
    "    print(pretrained_performance.round(4))\n",
    "    \n",
    "    best_idx = results_df['best_val_iou'].idxmax()\n",
    "    best_config = results_df.loc[best_idx]\n",
    "    print(f\"\\nBEST CONFIGURATION:\")\n",
    "    print(f\"Config: {best_config['config']}\")\n",
    "    print(f\"IoU: {best_config['best_val_iou']:.4f}\")\n",
    "    print(f\"Dice: {best_config['final_val_dice']:.4f}\")\n",
    "    \n",
    "    return encoder_performance, decoder_performance, pretrained_performance\n",
    "\n",
    "def statistical_analysis_almgsi(results_df):\n",
    "    print(\"STATISTICAL ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    resnet50_ious = results_df[results_df['encoder'] == 'resnet50']['best_val_iou']\n",
    "    se_resnext_ious = results_df[results_df['encoder'] == 'se_resnext101']['best_val_iou']\n",
    "    \n",
    "    if len(resnet50_ious) > 0 and len(se_resnext_ious) > 0:\n",
    "        t_stat, p_val = stats.ttest_ind(resnet50_ious, se_resnext_ious)\n",
    "        print(f\"Encoder comparison (ResNet50 vs SE-ResNeXt101):\")\n",
    "        print(f\"  t-statistic: {t_stat:.4f}, p-value: {p_val:.4f}\")\n",
    "        if p_val < 0.05:\n",
    "            print(f\"  Significant difference (p < 0.05)\")\n",
    "        else:\n",
    "            print(f\"  No significant difference (p >= 0.05)\")\n",
    "    \n",
    "    imagenet_ious = results_df[results_df['pretrained'] == 'imagenet']['best_val_iou']\n",
    "    micronet_ious = results_df[results_df['pretrained'] == 'micronet']['best_val_iou']\n",
    "    \n",
    "    if len(imagenet_ious) > 0 and len(micronet_ious) > 0:\n",
    "        t_stat, p_val = stats.ttest_ind(imagenet_ious, micronet_ious)\n",
    "        print(f\"\\nPretraining comparison (ImageNet vs MicroNet):\")\n",
    "        print(f\"  t-statistic: {t_stat:.4f}, p-value: {p_val:.4f}\")\n",
    "        if p_val < 0.05:\n",
    "            print(f\"  Significant difference (p < 0.05)\")\n",
    "            if micronet_ious.mean() > imagenet_ious.mean():\n",
    "                print(f\"  -> MicroNet performs significantly better\")\n",
    "            else:\n",
    "                print(f\"  -> ImageNet performs significantly better\")\n",
    "        else:\n",
    "            print(f\"  No significant difference (p >= 0.05)\")\n",
    "    \n",
    "    decoder_groups = [group['best_val_iou'].values for name, group in results_df.groupby('decoder')]\n",
    "    if len(decoder_groups) >= 2 and all(len(group) > 0 for group in decoder_groups):\n",
    "        f_stat, p_val = stats.f_oneway(*decoder_groups)\n",
    "        print(f\"\\nDecoder comparison (ANOVA):\")\n",
    "        print(f\"  F-statistic: {f_stat:.4f}, p-value: {p_val:.4f}\")\n",
    "        if p_val < 0.05:\n",
    "            print(f\"  Significant differences among decoders (p < 0.05)\")\n",
    "        else:\n",
    "            print(f\"  No significant differences among decoders (p >= 0.05)\")\n",
    "\n",
    "def main_almgsi_experiment():\n",
    "    print(\"MDSMG: Material Data Science Model Garden\")\n",
    "    print(\"Al-Mg-Si-XCT Dataset Experiment\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"Starting Al-Mg-Si-XCT experiment...\")\n",
    "    almgsi_results, almgsi_histories = run_almgsi_experiment()\n",
    "    \n",
    "    if len(almgsi_results) > 0:\n",
    "        print(\"\\nAnalyzing results...\")\n",
    "        encoder_perf, decoder_perf, pretrain_perf = analyze_almgsi_results(almgsi_results)\n",
    "        \n",
    "        print(\"\\nStatistical analysis...\")\n",
    "        statistical_analysis_almgsi(almgsi_results)\n",
    "        \n",
    "        print(\"\\nSaving detailed results...\")\n",
    "        encoder_perf.to_csv('almgsi_encoder_performance.csv')\n",
    "        decoder_perf.to_csv('almgsi_decoder_performance.csv')\n",
    "        pretrain_perf.to_csv('almgsi_pretraining_performance.csv')\n",
    "        \n",
    "        print(\"Al-Mg-Si-XCT analysis completed!\")\n",
    "        print(\"Files saved:\")\n",
    "        print(\"  - almgsi_experiment_results.csv\")\n",
    "        print(\"  - almgsi_encoder_performance.csv\")\n",
    "        print(\"  - almgsi_decoder_performance.csv\") \n",
    "        print(\"  - almgsi_pretraining_performance.csv\")\n",
    "        \n",
    "        return almgsi_results, almgsi_histories\n",
    "    else:\n",
    "        print(\"No successful experiments to analyze\")\n",
    "        return None, None\n",
    "\n",
    "def inspect_almgsi_data():\n",
    "    print(\"AL-MG-SI-XCT DATA INSPECTION\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    image_paths, mask_paths = load_dataset_paths(\".\", \"Al-Mg-Si-XCT\")\n",
    "    \n",
    "    if len(image_paths) > 0:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "        fig.suptitle('Al-Mg-Si-XCT Sample Data', fontsize=16)\n",
    "        \n",
    "        sample_idx = 0\n",
    "        \n",
    "        # Load and process image with proper bit depth handling\n",
    "        if image_paths[sample_idx].endswith('.tif'):\n",
    "            image = cv2.imread(image_paths[sample_idx], cv2.IMREAD_UNCHANGED)\n",
    "            print(f\"Original image dtype: {image.dtype}, shape: {image.shape}\")\n",
    "            print(f\"Image value range: {image.min()} - {image.max()}\")\n",
    "            \n",
    "            if len(image.shape) == 3:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            elif len(image.shape) == 2:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "            \n",
    "            # Use percentile-based normalization for better contrast\n",
    "            if image.dtype == np.uint16:\n",
    "                p1, p99 = np.percentile(image, (1, 99))\n",
    "                image_display = np.clip((image - p1) / (p99 - p1) * 255, 0, 255).astype(np.uint8)\n",
    "                print(f\"Applied percentile normalization: p1={p1}, p99={p99}\")\n",
    "            else:\n",
    "                image_display = image.astype(np.uint8)\n",
    "        else:\n",
    "            image_display = np.array(Image.open(image_paths[sample_idx]).convert('RGB'))\n",
    "        \n",
    "        # Load and process mask\n",
    "        if mask_paths[sample_idx].endswith('.tif'):\n",
    "            mask = cv2.imread(mask_paths[sample_idx], cv2.IMREAD_GRAYSCALE)\n",
    "            print(f\"Original mask dtype: {mask.dtype}, shape: {mask.shape}\")\n",
    "            print(f\"Mask value range: {mask.min()} - {mask.max()}\")\n",
    "            print(f\"Mask unique values: {np.unique(mask)}\")\n",
    "            \n",
    "            if mask.dtype == np.uint16:\n",
    "                mask_display = (mask / 65535.0 * 255.0).astype(np.uint8)\n",
    "            else:\n",
    "                mask_display = mask.astype(np.uint8)\n",
    "        else:\n",
    "            mask_display = np.array(Image.open(mask_paths[sample_idx]).convert('L'))\n",
    "        \n",
    "        # Calculate mask statistics\n",
    "        mask_binary = (mask_display > 127).astype(np.uint8)\n",
    "        total_pixels = mask_binary.size\n",
    "        positive_pixels = np.sum(mask_binary)\n",
    "        negative_pixels = total_pixels - positive_pixels\n",
    "        imbalance_ratio = negative_pixels / max(positive_pixels, 1)\n",
    "        \n",
    "        print(f\"Mask statistics:\")\n",
    "        print(f\"  Total pixels: {total_pixels}\")\n",
    "        print(f\"  Positive pixels: {positive_pixels} ({positive_pixels/total_pixels*100:.2f}%)\")\n",
    "        print(f\"  Negative pixels: {negative_pixels} ({negative_pixels/total_pixels*100:.2f}%)\")\n",
    "        print(f\"  Imbalance ratio: {imbalance_ratio:.1f}:1\")\n",
    "        \n",
    "        # Display images\n",
    "        axes[0].imshow(image_display)\n",
    "        axes[0].set_title('Al-Mg-Si-XCT - Original\\n(Percentile normalized)')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(mask_display, cmap='gray')\n",
    "        axes[1].set_title(f'Al-Mg-Si-XCT - Mask\\n({positive_pixels/total_pixels*100:.2f}% positive)')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # Create better overlay\n",
    "        overlay = image_display.copy().astype(np.float32) / 255.0\n",
    "        # Highlight positive regions in red\n",
    "        overlay[:, :, 0] = np.where(mask_binary > 0, 1.0, overlay[:, :, 0])\n",
    "        overlay[:, :, 1] = np.where(mask_binary > 0, 0.0, overlay[:, :, 1])\n",
    "        overlay[:, :, 2] = np.where(mask_binary > 0, 0.0, overlay[:, :, 2])\n",
    "        \n",
    "        axes[2].imshow(np.clip(overlay, 0, 1))\n",
    "        axes[2].set_title(f'Al-Mg-Si-XCT - Overlay\\nRatio: {imbalance_ratio:.1f}:1')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        print(f\"Found {len(image_paths)} Al-Mg-Si-XCT image-mask pairs\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('almgsi_sample_data.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No Al-Mg-Si-XCT data found\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU Memory available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(\"Starting Al-Mg-Si-XCT data inspection...\")\n",
    "inspect_almgsi_data()\n",
    "\n",
    "print(\"\\nStarting Al-Mg-Si-XCT experiment...\")\n",
    "almgsi_results, almgsi_histories = main_almgsi_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8851c9a2-f93c-427a-a222-506367d6c2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.2+cu118\n",
      "CUDA available: True\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 2080 Ti\n",
      "Loading dataset: Al-Mg-Si-XCT\n",
      "Image directory: ./Al-Mg-Si-XCT/images\n",
      "Mask directory: ./Al-Mg-Si-XCT/masks\n",
      "Found 150 valid image-mask pairs\n",
      "Loading dataset: AFM\n",
      "Image directory: ./AFM/images\n",
      "Mask directory: ./AFM/masks\n",
      "Found 46 valid image-mask pairs\n",
      "Loading dataset: Bond-Wire-XCT\n",
      "Image directory: ./Bond-Wire-XCT/images\n",
      "Mask directory: ./Bond-Wire-XCT/masks\n",
      "Found 0 valid image-mask pairs\n",
      "Loading dataset: Carbon\n",
      "Image directory: ./Carbon/images\n",
      "Mask directory: ./Carbon/masks\n",
      "Found 0 valid image-mask pairs\n",
      "Loading dataset: Fractography\n",
      "Image directory: ./Fractography/images\n",
      "Mask directory: ./Fractography/masks\n",
      "Found 0 valid image-mask pairs\n",
      "\n",
      "Dataset Summary:\n",
      "Al-Mg-Si-XCT: 150 image-mask pairs\n",
      "AFM: 46 image-mask pairs\n",
      "Bond-Wire-XCT: 0 image-mask pairs\n",
      "Carbon: 0 image-mask pairs\n",
      "Fractography: 0 image-mask pairs\n",
      "Testing MicroNet model loading...\n",
      "Successfully loaded MicroNet weights for micronet-resnet50 from resnet50_micronet_weights.pth\n",
      "Successfully loaded MicroNet weights for micronet-se_resnext101 from resnext101_micronet_weights.pth\n",
      "✓ MicroNet ResNet50 weights loaded successfully\n",
      "  Keys in state dict: 318\n",
      "✓ MicroNet SE-ResNeXt101 weights loaded successfully\n",
      "  Keys in state dict: 624\n",
      "GPU Memory available: 10.6 GB\n",
      "Starting Al-Mg-Si-XCT class imbalance analysis...\n",
      "ANALYZING CLASS IMBALANCE IN AL-MG-SI-XCT DATASET\n",
      "============================================================\n",
      "Loading dataset: Al-Mg-Si-XCT\n",
      "Image directory: ./Al-Mg-Si-XCT/images\n",
      "Mask directory: ./Al-Mg-Si-XCT/masks\n",
      "Found 150 valid image-mask pairs\n",
      "Analyzing 150 masks...\n",
      "Mask 1:     27/ 65536 positive ( 0.04%) - Ratio: 2426.3:1\n",
      "Mask 2:     24/ 65536 positive ( 0.04%) - Ratio: 2729.7:1\n",
      "Mask 3:     16/ 65536 positive ( 0.02%) - Ratio: 4095.0:1\n",
      "Mask 4:     29/ 65536 positive ( 0.04%) - Ratio: 2258.9:1\n",
      "Mask 5:     42/ 65536 positive ( 0.06%) - Ratio: 1559.4:1\n",
      "Mask 6:     39/ 65536 positive ( 0.06%) - Ratio: 1679.4:1\n",
      "Mask 7:     35/ 65536 positive ( 0.05%) - Ratio: 1871.5:1\n",
      "Mask 8:     31/ 65536 positive ( 0.05%) - Ratio: 2113.1:1\n",
      "Mask 9:     33/ 65536 positive ( 0.05%) - Ratio: 1984.9:1\n",
      "Mask 10:     38/ 65536 positive ( 0.06%) - Ratio: 1723.6:1\n",
      "\n",
      "OVERALL STATISTICS:\n",
      "Total pixels analyzed: 655,360\n",
      "Total positive pixels: 314.0 (0.048%)\n",
      "Overall imbalance ratio: 2086.1:1\n",
      "Average imbalance ratio: 2244.2:1\n",
      "\n",
      "RECOMMENDED LOSS FUNCTION SETTINGS:\n",
      "- Focal Loss gamma: 4.0-5.0 (high focus on hard examples)\n",
      "- Focal Loss alpha: 0.9-0.95 (high weight for positive class)\n",
      "- BCE pos_weight: 1000 (capped at 1000)\n",
      "- Tversky Loss beta (FN weight): 0.7-0.8 (high recall focus)\n",
      "\n",
      "Starting Al-Mg-Si-XCT data inspection...\n",
      "AL-MG-SI-XCT DATA INSPECTION\n",
      "----------------------------------------\n",
      "Loading dataset: Al-Mg-Si-XCT\n",
      "Image directory: ./Al-Mg-Si-XCT/images\n",
      "Mask directory: ./Al-Mg-Si-XCT/masks\n",
      "Found 150 valid image-mask pairs\n",
      "Sample 1 - Mask statistics:\n",
      "  Total pixels: 6300100\n",
      "  Positive pixels: 2720 (0.04%)\n",
      "  Negative pixels: 6297380.0 (99.96%)\n",
      "  Imbalance ratio: 2315.2:1\n",
      "Sample 2 - Mask statistics:\n",
      "  Total pixels: 6300100\n",
      "  Positive pixels: 2039 (0.03%)\n",
      "  Negative pixels: 6298061.0 (99.97%)\n",
      "  Imbalance ratio: 3088.8:1\n",
      "Found 150 Al-Mg-Si-XCT image-mask pairs\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Al-Mg-Si-XCT quick test with improved loss functions...\n",
      "Starting MDSMG Al-Mg-Si-XCT Quick Test (10 epochs) on cuda\n",
      "Total experiments to run: 4\n",
      "Running experiment 1/4: resnet50_unet_imagenet\n",
      "EXPERIMENT: Al-Mg-Si-XCT | resnet50 | unet | imagenet\n",
      "================================================================================\n",
      "Loading dataset: Al-Mg-Si-XCT\n",
      "Image directory: ./Al-Mg-Si-XCT/images\n",
      "Mask directory: ./Al-Mg-Si-XCT/masks\n",
      "Found 150 valid image-mask pairs\n",
      "Dataset split: 120 train, 30 validation\n",
      "Creating encoder: resnet50 with imagenet pretraining\n",
      "✓ ResNet50 with ImageNet weights loaded\n",
      "Training resnet50_unet_imagenet for 10 epochs on cuda\n",
      "Using Combined Loss (Enhanced Focal + Tversky) for extreme class imbalance\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 30/30 [00:41<00:00,  1.40s/it, train_loss=0.3993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: Train Loss: 0.4000 | Val Loss: 0.4006 | Val IoU: 0.0238 | Val Dice: 0.0464\n",
      "  ✓ New best IoU: 0.0238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 30/30 [00:41<00:00,  1.40s/it, train_loss=0.3991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2: Train Loss: 0.3990 | Val Loss: 0.3989 | Val IoU: 0.0326 | Val Dice: 0.0631\n",
      "  ✓ New best IoU: 0.0326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 30/30 [00:41<00:00,  1.39s/it, train_loss=0.3988]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3: Train Loss: 0.3986 | Val Loss: 0.3985 | Val IoU: 0.0337 | Val Dice: 0.0650\n",
      "  ✓ New best IoU: 0.0337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 30/30 [00:41<00:00,  1.39s/it, train_loss=0.3983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4: Train Loss: 0.3983 | Val Loss: 0.3982 | Val IoU: 0.0384 | Val Dice: 0.0738\n",
      "  ✓ New best IoU: 0.0384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 30/30 [00:41<00:00,  1.40s/it, train_loss=0.3975]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5: Train Loss: 0.3979 | Val Loss: 0.3977 | Val IoU: 0.0483 | Val Dice: 0.0919\n",
      "  ✓ New best IoU: 0.0483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 30/30 [00:41<00:00,  1.39s/it, train_loss=0.3969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6: Train Loss: 0.3973 | Val Loss: 0.3973 | Val IoU: 0.0725 | Val Dice: 0.1348\n",
      "  ✓ New best IoU: 0.0725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 30/30 [00:41<00:00,  1.39s/it, train_loss=0.3958]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7: Train Loss: 0.3965 | Val Loss: 0.3964 | Val IoU: 0.0553 | Val Dice: 0.1045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 30/30 [00:41<00:00,  1.39s/it, train_loss=0.3952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8: Train Loss: 0.3949 | Val Loss: 0.3938 | Val IoU: 0.0462 | Val Dice: 0.0881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 30/30 [00:41<00:00,  1.39s/it, train_loss=0.3897]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9: Train Loss: 0.3915 | Val Loss: 0.3897 | Val IoU: 0.0775 | Val Dice: 0.1434\n",
      "  ✓ New best IoU: 0.0775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 30/30 [00:41<00:00,  1.39s/it, train_loss=0.3719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.3847 | Val Loss: 0.3859 | Val IoU: 0.0685 | Val Dice: 0.1278\n",
      "Training completed! Best validation IoU: 0.0775\n",
      "Final validation Dice: 0.1278\n",
      "RESULTS: IoU=0.0775 | Dice=0.1278\n",
      "Experiment 1/4 completed: resnet50_unet_imagenet\n",
      "  Best IoU: 0.0775\n",
      "  Best Dice: 0.1278\n",
      "------------------------------------------------------------\n",
      "Running experiment 2/4: resnet50_unet_micronet\n",
      "EXPERIMENT: Al-Mg-Si-XCT | resnet50 | unet | micronet\n",
      "================================================================================\n",
      "Loading dataset: Al-Mg-Si-XCT\n",
      "Image directory: ./Al-Mg-Si-XCT/images\n",
      "Mask directory: ./Al-Mg-Si-XCT/masks\n",
      "Found 150 valid image-mask pairs\n",
      "Dataset split: 120 train, 30 validation\n",
      "Creating encoder: resnet50 with micronet pretraining\n",
      "Successfully loaded MicroNet weights for micronet-resnet50 from resnet50_micronet_weights.pth\n",
      "✓ ResNet50 with MicroNet weights loaded\n",
      "Training resnet50_unet_micronet for 10 epochs on cuda\n",
      "Using Combined Loss (Enhanced Focal + Tversky) for extreme class imbalance\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 30/30 [00:41<00:00,  1.39s/it, train_loss=0.4004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: Train Loss: 0.4019 | Val Loss: 0.4012 | Val IoU: 0.0083 | Val Dice: 0.0165\n",
      "  ✓ New best IoU: 0.0083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 30/30 [00:41<00:00,  1.39s/it, train_loss=0.3998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2: Train Loss: 0.4002 | Val Loss: 0.4000 | Val IoU: 0.0139 | Val Dice: 0.0273\n",
      "  ✓ New best IoU: 0.0139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 30/30 [00:41<00:00,  1.39s/it, train_loss=0.3995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3: Train Loss: 0.3998 | Val Loss: 0.3997 | Val IoU: 0.0149 | Val Dice: 0.0293\n",
      "  ✓ New best IoU: 0.0149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 30/30 [00:41<00:00,  1.39s/it, train_loss=0.3995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4: Train Loss: 0.3996 | Val Loss: 0.3994 | Val IoU: 0.0159 | Val Dice: 0.0313\n",
      "  ✓ New best IoU: 0.0159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 30/30 [00:41<00:00,  1.39s/it, train_loss=0.3994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5: Train Loss: 0.3993 | Val Loss: 0.3993 | Val IoU: 0.0150 | Val Dice: 0.0294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 30/30 [00:41<00:00,  1.39s/it, train_loss=0.3992]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6: Train Loss: 0.3990 | Val Loss: 0.3990 | Val IoU: 0.0186 | Val Dice: 0.0364\n",
      "  ✓ New best IoU: 0.0186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 30/30 [00:41<00:00,  1.39s/it, train_loss=0.3983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7: Train Loss: 0.3987 | Val Loss: 0.3987 | Val IoU: 0.0218 | Val Dice: 0.0426\n",
      "  ✓ New best IoU: 0.0218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 30/30 [00:41<00:00,  1.39s/it, train_loss=0.3978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8: Train Loss: 0.3984 | Val Loss: 0.3982 | Val IoU: 0.0405 | Val Dice: 0.0777\n",
      "  ✓ New best IoU: 0.0405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 30/30 [00:41<00:00,  1.39s/it, train_loss=0.3980]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9: Train Loss: 0.3981 | Val Loss: 0.3980 | Val IoU: 0.0261 | Val Dice: 0.0507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 30/30 [00:41<00:00,  1.39s/it, train_loss=0.3973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.3977 | Val Loss: 0.3976 | Val IoU: 0.0231 | Val Dice: 0.0451\n",
      "Training completed! Best validation IoU: 0.0405\n",
      "Final validation Dice: 0.0451\n",
      "RESULTS: IoU=0.0405 | Dice=0.0451\n",
      "Experiment 2/4 completed: resnet50_unet_micronet\n",
      "  Best IoU: 0.0405\n",
      "  Best Dice: 0.0451\n",
      "------------------------------------------------------------\n",
      "Running experiment 3/4: resnet50_deeplabv3plus_imagenet\n",
      "EXPERIMENT: Al-Mg-Si-XCT | resnet50 | deeplabv3plus | imagenet\n",
      "================================================================================\n",
      "Loading dataset: Al-Mg-Si-XCT\n",
      "Image directory: ./Al-Mg-Si-XCT/images\n",
      "Mask directory: ./Al-Mg-Si-XCT/masks\n",
      "Found 150 valid image-mask pairs\n",
      "Dataset split: 120 train, 30 validation\n",
      "Creating encoder: resnet50 with imagenet pretraining\n",
      "✓ ResNet50 with ImageNet weights loaded\n",
      "Training resnet50_deeplabv3plus_imagenet for 10 epochs on cuda\n",
      "Using Combined Loss (Enhanced Focal + Tversky) for extreme class imbalance\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  30%|███       | 9/30 [00:13<00:31,  1.49s/it, train_loss=0.4009]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 965\u001b[0m\n\u001b[1;32m    962\u001b[0m inspect_almgsi_data()\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting Al-Mg-Si-XCT quick test with improved loss functions...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 965\u001b[0m almgsi_results, almgsi_histories \u001b[38;5;241m=\u001b[39m \u001b[43mrun_almgsi_quick_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 764\u001b[0m, in \u001b[0;36mrun_almgsi_quick_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m    760\u001b[0m experiment_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning experiment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_experiments\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecoder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 764\u001b[0m result, history \u001b[38;5;241m=\u001b[39m \u001b[43mrun_single_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m history:\n\u001b[1;32m    769\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[0;32mIn[11], line 719\u001b[0m, in \u001b[0;36mrun_single_experiment\u001b[0;34m(dataset_name, encoder_name, decoder_name, pretrained, device)\u001b[0m\n\u001b[1;32m    716\u001b[0m model \u001b[38;5;241m=\u001b[39m MaterialSegmentationModel(encoder_name, decoder_name, pretrained, n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    718\u001b[0m \u001b[38;5;66;03m# Only 10 epochs for quick testing\u001b[39;00m\n\u001b[0;32m--> 719\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m result \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset_name,\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m'\u001b[39m: encoder_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_val_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_losses\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    733\u001b[0m }\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRESULTS: IoU=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_val_iou\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Dice=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_val_dice\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 609\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, device)\u001b[0m\n\u001b[1;32m    606\u001b[0m train_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    608\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 609\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m    610\u001b[0m     images, masks \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), masks\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    612\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[11], line 62\u001b[0m, in \u001b[0;36mMaterialDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39muint16:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Use percentile-based normalization for better contrast\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     p1, p99 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpercentile(image, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m99\u001b[39m))\n\u001b[0;32m---> 62\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mp99\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39muint8:\n\u001b[1;32m     64\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:2096\u001b[0m, in \u001b[0;36m_clip_dispatcher\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2034\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;124;03m    Return selected slices of an array along given axis.\u001b[39;00m\n\u001b[1;32m   2036\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2091\u001b[0m \n\u001b[1;32m   2092\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2093\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompress\u001b[39m\u001b[38;5;124m'\u001b[39m, condition, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout)\n\u001b[0;32m-> 2096\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_clip_dispatcher\u001b[39m(a, a_min, a_max, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2097\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, a_min, a_max)\n\u001b[1;32m   2100\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_clip_dispatcher)\n\u001b[1;32m   2101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip\u001b[39m(a, a_min, a_max, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Material Data Science Model Garden (MDSMG) Experiment\n",
    "# Systematic Evaluation of Encoder-Decoder Architectures for Material Image Segmentation\n",
    "# Al-Mg-Si-XCT Dataset Experiment - IMPROVED FOR CLASS IMBALANCE\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "import segmentation_models_pytorch as smp\n",
    "from huggingface_hub import hf_hub_download\n",
    "import timm\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current device:\", torch.cuda.current_device())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name())\n",
    "\n",
    "class MaterialDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transform=None, image_size=(256, 256)):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform\n",
    "        self.image_size = image_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "        \n",
    "        # Load image with proper handling for different bit depths\n",
    "        if image_path.endswith('.tif') or image_path.endswith('.tiff'):\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "            \n",
    "            if len(image.shape) == 3:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            elif len(image.shape) == 2:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "            \n",
    "            # Handle different bit depths (uint16, uint8, etc.)\n",
    "            if image.dtype == np.uint16:\n",
    "                # Use percentile-based normalization for better contrast\n",
    "                p1, p99 = np.percentile(image, (1, 99))\n",
    "                image = np.clip((image - p1) / (p99 - p1) * 255, 0, 255).astype(np.uint8)\n",
    "            elif image.dtype != np.uint8:\n",
    "                image = image.astype(np.uint8)\n",
    "                \n",
    "        else:\n",
    "            image = np.array(Image.open(image_path).convert('RGB'))\n",
    "            \n",
    "        # Load mask with proper handling\n",
    "        if mask_path.endswith('.tif') or mask_path.endswith('.tiff'):\n",
    "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            # Handle different mask bit depths\n",
    "            if mask.dtype == np.uint16:\n",
    "                mask_max = mask.max()\n",
    "                if mask_max > 255:\n",
    "                    mask = (mask.astype(np.float32) / mask_max * 255.0).astype(np.uint8)\n",
    "                else:\n",
    "                    mask = mask.astype(np.uint8)\n",
    "            elif mask.dtype != np.uint8:\n",
    "                mask = mask.astype(np.uint8)\n",
    "        else:\n",
    "            mask = np.array(Image.open(mask_path).convert('L'))\n",
    "        \n",
    "        # Resize images\n",
    "        image = cv2.resize(image, self.image_size)\n",
    "        mask = cv2.resize(mask, self.image_size, interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        # More careful mask binarization\n",
    "        unique_vals = np.unique(mask)\n",
    "        \n",
    "        # Create binary mask\n",
    "        if len(unique_vals) == 2 and 0 in unique_vals:\n",
    "            mask = (mask > 0).astype(np.uint8)\n",
    "        else:\n",
    "            threshold = np.percentile(mask[mask > 0], 50) if np.any(mask > 0) else 127\n",
    "            mask = (mask > threshold).astype(np.uint8)\n",
    "        \n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "        else:\n",
    "            # Convert to float32 before creating tensor\n",
    "            image = image.astype(np.float32)\n",
    "            image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n",
    "            mask = torch.from_numpy(mask.astype(np.float32)).float()\n",
    "            \n",
    "        return image, mask\n",
    "\n",
    "def load_dataset_paths(base_path, dataset_name):\n",
    "    dataset_path = os.path.join(base_path, dataset_name)\n",
    "    image_dir = os.path.join(dataset_path, 'images')\n",
    "    mask_dir = os.path.join(dataset_path, 'masks')\n",
    "    \n",
    "    print(f\"Loading dataset: {dataset_name}\")\n",
    "    print(f\"Image directory: {image_dir}\")\n",
    "    print(f\"Mask directory: {mask_dir}\")\n",
    "    \n",
    "    if not os.path.exists(image_dir) or not os.path.exists(mask_dir):\n",
    "        print(f\"Warning: Directory does not exist for {dataset_name}\")\n",
    "        return [], []\n",
    "    \n",
    "    image_extensions = ['*.png', '*.tif', '*.tiff', '*.jpg', '*.jpeg']\n",
    "    image_paths = []\n",
    "    for ext in image_extensions:\n",
    "        image_paths.extend(glob.glob(os.path.join(image_dir, ext)))\n",
    "    \n",
    "    mask_paths = []\n",
    "    for img_path in image_paths:\n",
    "        img_name = os.path.basename(img_path)\n",
    "        img_name_no_ext = os.path.splitext(img_name)[0]\n",
    "        \n",
    "        for mask_ext in ['png', 'tif', 'tiff']:\n",
    "            mask_path = os.path.join(mask_dir, f\"{img_name_no_ext}.{mask_ext}\")\n",
    "            if os.path.exists(mask_path):\n",
    "                mask_paths.append(mask_path)\n",
    "                break\n",
    "    \n",
    "    valid_pairs = []\n",
    "    valid_mask_paths = []\n",
    "    for img_path, mask_path in zip(image_paths, mask_paths):\n",
    "        if os.path.exists(img_path) and os.path.exists(mask_path):\n",
    "            valid_pairs.append(img_path)\n",
    "            valid_mask_paths.append(mask_path)\n",
    "    \n",
    "    print(f\"Found {len(valid_pairs)} valid image-mask pairs\")\n",
    "    return valid_pairs, valid_mask_paths\n",
    "\n",
    "base_path = \".\"\n",
    "datasets = ['Al-Mg-Si-XCT', 'AFM', 'Bond-Wire-XCT', 'Carbon', 'Fractography']\n",
    "\n",
    "dataset_info = {}\n",
    "for dataset_name in datasets:\n",
    "    image_paths, mask_paths = load_dataset_paths(base_path, dataset_name)\n",
    "    dataset_info[dataset_name] = len(image_paths)\n",
    "    \n",
    "print(\"\\nDataset Summary:\")\n",
    "for name, count in dataset_info.items():\n",
    "    print(f\"{name}: {count} image-mask pairs\")\n",
    "\n",
    "def load_micronet_weights(model_name='micronet-resnet50'):\n",
    "    try:\n",
    "        if model_name == 'micronet-resnet50':\n",
    "            filenames_to_try = [\n",
    "                \"resnet50_micronet_weights.pth\",\n",
    "                \"resnet50_imagenet-micronet_weights.pth\", \n",
    "                \"pytorch_model.bin\"\n",
    "            ]\n",
    "            repo_id = \"jstuckner/microscopy-resnet50-micronet\"\n",
    "            \n",
    "        elif model_name == 'micronet-se_resnext101':\n",
    "            filenames_to_try = [\n",
    "                \"resnext101_micronet_weights.pth\",\n",
    "                \"resnext101_imagenet-micronet_weights.pth\",\n",
    "                \"pytorch_model.bin\"\n",
    "            ]\n",
    "            repo_id = \"jstuckner/microscopy-resnext101-micronet\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown MicroNet model: {model_name}\")\n",
    "        \n",
    "        weights = None\n",
    "        for filename in filenames_to_try:\n",
    "            try:\n",
    "                model_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "                weights = torch.load(model_path, map_location='cpu')\n",
    "                print(f\"Successfully loaded MicroNet weights for {model_name} from {filename}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {filename}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if weights is None:\n",
    "            print(f\"Failed to load any weights for {model_name}\")\n",
    "            \n",
    "        return weights\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load MicroNet weights for {model_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"Testing MicroNet model loading...\")\n",
    "micronet_resnet50_weights = load_micronet_weights('micronet-resnet50')\n",
    "micronet_se_resnext101_weights = load_micronet_weights('micronet-se_resnext101')\n",
    "\n",
    "if micronet_resnet50_weights:\n",
    "    print(\"✓ MicroNet ResNet50 weights loaded successfully\")\n",
    "    print(f\"  Keys in state dict: {len(micronet_resnet50_weights.keys())}\")\n",
    "    \n",
    "if micronet_se_resnext101_weights:\n",
    "    print(\"✓ MicroNet SE-ResNeXt101 weights loaded successfully\")\n",
    "    print(f\"  Keys in state dict: {len(micronet_se_resnext101_weights.keys())}\")\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.squeeze(x).view(b, c)\n",
    "        y = self.excitation(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "def create_encoder(encoder_name='resnet50', pretrained='imagenet'):\n",
    "    print(f\"Creating encoder: {encoder_name} with {pretrained} pretraining\")\n",
    "    \n",
    "    if encoder_name == 'resnet50':\n",
    "        if pretrained == 'imagenet':\n",
    "            encoder = smp.encoders.get_encoder('resnet50', in_channels=3, weights='imagenet')\n",
    "            print(\"✓ ResNet50 with ImageNet weights loaded\")\n",
    "        elif pretrained == 'micronet':\n",
    "            encoder = smp.encoders.get_encoder('resnet50', in_channels=3, weights=None)\n",
    "            micronet_weights = load_micronet_weights('micronet-resnet50')\n",
    "            if micronet_weights:\n",
    "                try:\n",
    "                    encoder.load_state_dict(micronet_weights, strict=False)\n",
    "                    print(\"✓ ResNet50 with MicroNet weights loaded\")\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠ Failed to load MicroNet weights strictly, trying partial loading: {str(e)}\")\n",
    "                    model_dict = encoder.state_dict()\n",
    "                    pretrained_dict = {k: v for k, v in micronet_weights.items() \n",
    "                                     if k in model_dict and v.shape == model_dict[k].shape}\n",
    "                    model_dict.update(pretrained_dict)\n",
    "                    encoder.load_state_dict(model_dict)\n",
    "                    print(f\"✓ ResNet50 with partial MicroNet weights loaded ({len(pretrained_dict)} layers)\")\n",
    "            else:\n",
    "                print(\"⚠ Failed to load MicroNet weights, using random initialization\")\n",
    "        else:\n",
    "            encoder = smp.encoders.get_encoder('resnet50', in_channels=3, weights=None)\n",
    "            print(\"✓ ResNet50 with random weights\")\n",
    "            \n",
    "        encoder_channels = [3, 64, 256, 512, 1024, 2048]\n",
    "    \n",
    "    elif encoder_name == 'se_resnext101':\n",
    "        if pretrained == 'imagenet':\n",
    "            encoder = smp.encoders.get_encoder('se_resnext101_32x4d', in_channels=3, weights='imagenet')\n",
    "            print(\"✓ SE-ResNeXt101 with ImageNet weights loaded\")\n",
    "        elif pretrained == 'micronet':\n",
    "            print(\"⚠ MicroNet weights for SE-ResNeXt101 have architecture mismatch\")\n",
    "            print(\"  Using ImageNet pretrained weights as fallback\")\n",
    "            encoder = smp.encoders.get_encoder('se_resnext101_32x4d', in_channels=3, weights='imagenet')\n",
    "            \n",
    "            micronet_weights = load_micronet_weights('micronet-se_resnext101')\n",
    "            if micronet_weights:\n",
    "                try:\n",
    "                    model_dict = encoder.state_dict()\n",
    "                    pretrained_dict = {}\n",
    "                    for k, v in micronet_weights.items():\n",
    "                        if k in model_dict and v.shape == model_dict[k].shape:\n",
    "                            pretrained_dict[k] = v\n",
    "                    \n",
    "                    if len(pretrained_dict) > 0:\n",
    "                        model_dict.update(pretrained_dict)\n",
    "                        encoder.load_state_dict(model_dict)\n",
    "                        print(f\"✓ Loaded {len(pretrained_dict)} compatible layers from MicroNet\")\n",
    "                    else:\n",
    "                        print(\"✗ No compatible layers found in MicroNet weights\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"✗ Failed to load any MicroNet weights: {str(e)}\")\n",
    "            \n",
    "        else:\n",
    "            encoder = smp.encoders.get_encoder('se_resnext101_32x4d', in_channels=3, weights=None)\n",
    "            print(\"✓ SE-ResNeXt101 with random weights\")\n",
    "            \n",
    "        encoder_channels = [3, 64, 256, 512, 1024, 2048]\n",
    "    \n",
    "    return encoder, encoder_channels\n",
    "\n",
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, encoder_channels, decoder_channels, n_classes=1):\n",
    "        super(UNetDecoder, self).__init__()\n",
    "        \n",
    "        self.decoder_blocks = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(decoder_channels)):\n",
    "            if i == 0:\n",
    "                in_ch = encoder_channels[-1]\n",
    "            else:\n",
    "                in_ch = decoder_channels[i-1]\n",
    "            \n",
    "            out_ch = decoder_channels[i]\n",
    "            skip_ch = encoder_channels[-(i+2)] if i < len(encoder_channels)-1 else 0\n",
    "            \n",
    "            block = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2),\n",
    "                nn.Conv2d(out_ch + skip_ch, out_ch, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "            \n",
    "            self.decoder_blocks.append(block)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(decoder_channels[-1], n_classes, kernel_size=1)\n",
    "        \n",
    "    def forward(self, encoder_features):\n",
    "        x = encoder_features[-1]\n",
    "        \n",
    "        for i, decoder_block in enumerate(self.decoder_blocks):\n",
    "            x = decoder_block[0](x)\n",
    "            \n",
    "            if i < len(encoder_features) - 1:\n",
    "                skip = encoder_features[-(i+2)]\n",
    "                if x.shape[2:] != skip.shape[2:]:\n",
    "                    x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
    "                x = torch.cat([x, skip], dim=1)\n",
    "            \n",
    "            for layer in decoder_block[1:]:\n",
    "                x = layer(x)\n",
    "        \n",
    "        return self.final_conv(x)\n",
    "\n",
    "class ASPPModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, atrous_rates):\n",
    "        super(ASPPModule, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.atrous_convs = nn.ModuleList()\n",
    "        for rate in atrous_rates:\n",
    "            self.atrous_convs.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, padding=rate, dilation=rate, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ))\n",
    "        \n",
    "        self.global_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(out_channels * (2 + len(atrous_rates)), out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[2:]\n",
    "        \n",
    "        feat1 = self.conv1(x)\n",
    "        atrous_feats = [conv(x) for conv in self.atrous_convs]\n",
    "        global_feat = self.global_pool(x)\n",
    "        global_feat = F.interpolate(global_feat, size=(h, w), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        concat_feat = torch.cat([feat1] + atrous_feats + [global_feat], dim=1)\n",
    "        return self.project(concat_feat)\n",
    "\n",
    "class DeepLabV3Decoder(nn.Module):\n",
    "    def __init__(self, encoder_channels, n_classes=1):\n",
    "        super(DeepLabV3Decoder, self).__init__()\n",
    "        self.aspp = ASPPModule(encoder_channels[-1], 256, [6, 12, 18])\n",
    "        self.classifier = nn.Conv2d(256, n_classes, 1)\n",
    "        \n",
    "    def forward(self, encoder_features):\n",
    "        x = encoder_features[-1]\n",
    "        x = self.aspp(x)\n",
    "        x = self.classifier(x)\n",
    "        x = F.interpolate(x, scale_factor=32, mode='bilinear', align_corners=False)\n",
    "        return x\n",
    "\n",
    "class DeepLabV3PlusDecoder(nn.Module):\n",
    "    def __init__(self, encoder_channels, n_classes=1):\n",
    "        super(DeepLabV3PlusDecoder, self).__init__()\n",
    "        \n",
    "        self.aspp = ASPPModule(encoder_channels[-1], 256, [6, 12, 18])\n",
    "        low_level_channels = encoder_channels[1]\n",
    "        \n",
    "        self.low_level_proj = nn.Sequential(\n",
    "            nn.Conv2d(low_level_channels, 48, 1, bias=False),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(256 + 48, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Conv2d(256, n_classes, 1)\n",
    "        \n",
    "    def forward(self, encoder_features):\n",
    "        low_level = encoder_features[1]\n",
    "        high_level = encoder_features[-1]\n",
    "        \n",
    "        high_level = self.aspp(high_level)\n",
    "        high_level = F.interpolate(high_level, size=low_level.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        low_level = self.low_level_proj(low_level)\n",
    "        \n",
    "        concat_feat = torch.cat([high_level, low_level], dim=1)\n",
    "        decoded = self.decoder(concat_feat)\n",
    "        output = self.classifier(decoded)\n",
    "        \n",
    "        output = F.interpolate(output, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class MaterialSegmentationModel(nn.Module):\n",
    "    def __init__(self, encoder_name='resnet50', decoder_name='unet', pretrained='imagenet', n_classes=1):\n",
    "        super(MaterialSegmentationModel, self).__init__()\n",
    "        \n",
    "        self.encoder, encoder_channels = create_encoder(encoder_name, pretrained)\n",
    "        \n",
    "        if decoder_name == 'unet':\n",
    "            self.decoder = UNetDecoder(encoder_channels[1:], [256, 128, 64, 32], n_classes)\n",
    "        elif decoder_name == 'deeplabv3':\n",
    "            self.decoder = DeepLabV3Decoder(encoder_channels[1:], n_classes)\n",
    "        elif decoder_name == 'deeplabv3plus':\n",
    "            self.decoder = DeepLabV3PlusDecoder(encoder_channels, n_classes)\n",
    "        \n",
    "        self.config = f\"{encoder_name}_{decoder_name}_{pretrained}\"\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoder_features = self.encoder(x)\n",
    "        \n",
    "        if hasattr(self.decoder, '__class__') and self.decoder.__class__.__name__ == 'DeepLabV3PlusDecoder':\n",
    "            output = self.decoder(encoder_features)\n",
    "        else:\n",
    "            output = self.decoder(encoder_features[1:])\n",
    "        \n",
    "        return output\n",
    "\n",
    "# IMPROVED LOSS FUNCTIONS FOR EXTREME CLASS IMBALANCE\n",
    "class AdaptiveWeightedBCELoss(nn.Module):\n",
    "    def __init__(self, min_pos_weight=100.0, max_pos_weight=1000.0):\n",
    "        super(AdaptiveWeightedBCELoss, self).__init__()\n",
    "        self.min_pos_weight = min_pos_weight\n",
    "        self.max_pos_weight = max_pos_weight\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        # Calculate dynamic positive weight based on class distribution\n",
    "        pos_pixels = targets.sum()\n",
    "        neg_pixels = (targets.numel() - pos_pixels)\n",
    "        \n",
    "        if pos_pixels > 0:\n",
    "            # Much higher weight for positive class\n",
    "            dynamic_pos_weight = (neg_pixels / pos_pixels).clamp(\n",
    "                min=self.min_pos_weight, \n",
    "                max=self.max_pos_weight\n",
    "            )\n",
    "        else:\n",
    "            dynamic_pos_weight = torch.tensor(self.min_pos_weight)\n",
    "            \n",
    "        print(f\"Adaptive pos_weight: {dynamic_pos_weight:.1f}\")\n",
    "        return F.binary_cross_entropy_with_logits(inputs, targets, \n",
    "                                                pos_weight=dynamic_pos_weight)\n",
    "\n",
    "# Enhanced Focal Loss with higher gamma for extreme imbalance\n",
    "class EnhancedFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.9, gamma=4.0):  # Increased alpha and gamma\n",
    "        super(EnhancedFocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        \n",
    "        # Apply alpha weighting - higher for positive class\n",
    "        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "        \n",
    "        focal_loss = alpha_t * (1-pt)**self.gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# Tversky Loss - better for imbalanced segmentation\n",
    "class TverskyLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.3, beta=0.7, smooth=1e-6):\n",
    "        super(TverskyLoss, self).__init__()\n",
    "        self.alpha = alpha  # Weight for false positives\n",
    "        self.beta = beta    # Weight for false negatives (higher for recall)\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "        \n",
    "        # Flatten tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        # True positives, false positives, false negatives\n",
    "        tp = (inputs * targets).sum()\n",
    "        fp = ((1 - targets) * inputs).sum()\n",
    "        fn = (targets * (1 - inputs)).sum()\n",
    "        \n",
    "        tversky = (tp + self.smooth) / (tp + self.alpha * fp + self.beta * fn + self.smooth)\n",
    "        return 1 - tversky\n",
    "\n",
    "# Combined Loss for extreme imbalance\n",
    "class CombinedImbalanceLoss(nn.Module):\n",
    "    def __init__(self, focal_weight=0.6, tversky_weight=0.4):\n",
    "        super(CombinedImbalanceLoss, self).__init__()\n",
    "        self.focal_loss = EnhancedFocalLoss(alpha=0.9, gamma=4.0)\n",
    "        self.tversky_loss = TverskyLoss(alpha=0.3, beta=0.7)\n",
    "        self.focal_weight = focal_weight\n",
    "        self.tversky_weight = tversky_weight\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        focal = self.focal_loss(inputs, targets)\n",
    "        tversky = self.tversky_loss(inputs, targets)\n",
    "        return self.focal_weight * focal + self.tversky_weight * tversky\n",
    "\n",
    "def dice_coefficient(pred, target, smooth=1e-6):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred = (pred > 0.5).float()\n",
    "    \n",
    "    # Ensure both tensors have same shape\n",
    "    if target.dim() == 3 and pred.dim() == 4:\n",
    "        target = target.unsqueeze(1)\n",
    "    elif target.dim() == 4 and pred.dim() == 3:\n",
    "        pred = pred.unsqueeze(1)\n",
    "    \n",
    "    intersection = (pred * target).sum(dim=(2, 3))\n",
    "    union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
    "    \n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return dice.mean()\n",
    "\n",
    "def iou_score(pred, target, smooth=1e-6):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred = (pred > 0.5).float()\n",
    "    \n",
    "    # Ensure both tensors have same shape\n",
    "    if target.dim() == 3 and pred.dim() == 4:\n",
    "        target = target.unsqueeze(1)\n",
    "    elif target.dim() == 4 and pred.dim() == 3:\n",
    "        pred = pred.unsqueeze(1)\n",
    "    \n",
    "    intersection = (pred * target).sum(dim=(2, 3))\n",
    "    union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3)) - intersection\n",
    "    \n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return iou.mean()\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, device='cuda'):  # Reduced to 10 epochs\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Higher learning rate for faster convergence in short training\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "    \n",
    "    # Use Combined Loss for extreme class imbalance\n",
    "    criterion = CombinedImbalanceLoss(focal_weight=0.6, tversky_weight=0.4)\n",
    "    \n",
    "    # More aggressive scheduler for short training\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5, verbose=True)\n",
    "    \n",
    "    best_val_iou = 0\n",
    "    history = {\n",
    "        'train_losses': [],\n",
    "        'val_losses': [],\n",
    "        'val_ious': [],\n",
    "        'val_dices': []\n",
    "    }\n",
    "    \n",
    "    print(f\"Training {model.config} for {num_epochs} epochs on {device}\")\n",
    "    print(f\"Using Combined Loss (Enhanced Focal + Tversky) for extreme class imbalance\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for images, masks in pbar:\n",
    "            images, masks = images.to(device), masks.to(device).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            if outputs.shape[2:] != masks.shape[1:]:\n",
    "                outputs = F.interpolate(outputs, size=masks.shape[1:], mode='bilinear', align_corners=False)\n",
    "            \n",
    "            outputs = outputs.squeeze(1)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "            \n",
    "            pbar.set_postfix({'train_loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            if train_batches % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_iou = 0\n",
    "        val_dice = 0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(device), masks.to(device).float()\n",
    "                \n",
    "                outputs = model(images)\n",
    "                if outputs.shape[2:] != masks.shape[1:]:\n",
    "                    outputs = F.interpolate(outputs, size=masks.shape[1:], mode='bilinear', align_corners=False)\n",
    "                \n",
    "                outputs = outputs.squeeze(1)\n",
    "                loss = criterion(outputs, masks)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Ensure consistent dimensions for metrics\n",
    "                if masks.dim() == 3:\n",
    "                    masks_for_metrics = masks.unsqueeze(1)\n",
    "                else:\n",
    "                    masks_for_metrics = masks\n",
    "                \n",
    "                if outputs.dim() == 3:\n",
    "                    outputs_for_metrics = outputs.unsqueeze(1)\n",
    "                else:\n",
    "                    outputs_for_metrics = outputs\n",
    "                \n",
    "                val_iou += iou_score(outputs_for_metrics, masks_for_metrics).item()\n",
    "                val_dice += dice_coefficient(outputs_for_metrics, masks_for_metrics).item()\n",
    "                val_batches += 1\n",
    "        \n",
    "        avg_train_loss = train_loss / train_batches\n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        avg_val_iou = val_iou / val_batches\n",
    "        avg_val_dice = val_dice / val_batches\n",
    "        \n",
    "        history['train_losses'].append(avg_train_loss)\n",
    "        history['val_losses'].append(avg_val_loss)\n",
    "        history['val_ious'].append(avg_val_iou)\n",
    "        history['val_dices'].append(avg_val_dice)\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Print results every epoch for 10-epoch training\n",
    "        print(f'Epoch {epoch+1:2d}: Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val IoU: {avg_val_iou:.4f} | Val Dice: {avg_val_dice:.4f}')\n",
    "        \n",
    "        if avg_val_iou > best_val_iou:\n",
    "            best_val_iou = avg_val_iou\n",
    "            torch.save(model.state_dict(), f'best_{model.config}.pth')\n",
    "            print(f'  ✓ New best IoU: {best_val_iou:.4f}')\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    history['best_val_iou'] = best_val_iou\n",
    "    print(f\"Training completed! Best validation IoU: {best_val_iou:.4f}\")\n",
    "    print(f\"Final validation Dice: {history['val_dices'][-1]:.4f}\")\n",
    "    return history\n",
    "\n",
    "def run_single_experiment(dataset_name, encoder_name, decoder_name, pretrained, device='cuda'):\n",
    "    print(f\"EXPERIMENT: {dataset_name} | {encoder_name} | {decoder_name} | {pretrained}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    image_paths, mask_paths = load_dataset_paths(\".\", dataset_name)\n",
    "    \n",
    "    if len(image_paths) < 4:\n",
    "        print(f\"Insufficient data for {dataset_name}: {len(image_paths)} samples\")\n",
    "        return None, None\n",
    "    \n",
    "    train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        image_paths, mask_paths, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset split: {len(train_images)} train, {len(val_images)} validation\")\n",
    "    \n",
    "    train_dataset = MaterialDataset(train_images, train_masks, image_size=(256, 256))\n",
    "    val_dataset = MaterialDataset(val_images, val_masks, image_size=(256, 256))\n",
    "    \n",
    "    # Increased batch size for faster training\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "    \n",
    "    model = MaterialSegmentationModel(encoder_name, decoder_name, pretrained, n_classes=1)\n",
    "    \n",
    "    # Only 10 epochs for quick testing\n",
    "    history = train_model(model, train_loader, val_loader, num_epochs=10, device=device)\n",
    "    \n",
    "    result = {\n",
    "        'dataset': dataset_name,\n",
    "        'encoder': encoder_name,\n",
    "        'decoder': decoder_name,\n",
    "        'pretrained': pretrained,\n",
    "        'config': f\"{encoder_name}_{decoder_name}_{pretrained}\",\n",
    "        'train_samples': len(train_images),\n",
    "        'val_samples': len(val_images),\n",
    "        'best_val_iou': history['best_val_iou'],\n",
    "        'final_val_dice': history['val_dices'][-1],\n",
    "        'final_train_loss': history['train_losses'][-1],\n",
    "        'final_val_loss': history['val_losses'][-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"RESULTS: IoU={result['best_val_iou']:.4f} | Dice={result['final_val_dice']:.4f}\")\n",
    "    \n",
    "    return result, history\n",
    "\n",
    "def run_almgsi_quick_test():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Starting MDSMG Al-Mg-Si-XCT Quick Test (10 epochs) on {device}\")\n",
    "    \n",
    "    datasets = ['Al-Mg-Si-XCT']\n",
    "    # Test with fewer configurations for quick results\n",
    "    encoders = ['resnet50']  # Start with ResNet50 only\n",
    "    decoders = ['unet', 'deeplabv3plus']  # Test best performing decoders\n",
    "    pretrained_options = ['imagenet', 'micronet']\n",
    "    \n",
    "    results = []\n",
    "    histories = {}\n",
    "    experiment_count = 0\n",
    "    \n",
    "    total_experiments = len(datasets) * len(encoders) * len(decoders) * len(pretrained_options)\n",
    "    print(f\"Total experiments to run: {total_experiments}\")\n",
    "    \n",
    "    for dataset_name in datasets:\n",
    "        for encoder in encoders:\n",
    "            for decoder in decoders:\n",
    "                for pretrained in pretrained_options:\n",
    "                    experiment_count += 1\n",
    "                    \n",
    "                    print(f\"Running experiment {experiment_count}/{total_experiments}: {encoder}_{decoder}_{pretrained}\")\n",
    "                    \n",
    "                    result, history = run_single_experiment(\n",
    "                        dataset_name, encoder, decoder, pretrained, device\n",
    "                    )\n",
    "                    \n",
    "                    if result and history:\n",
    "                        results.append(result)\n",
    "                        histories[result['config'] + '_' + dataset_name] = history\n",
    "                        print(f\"Experiment {experiment_count}/{total_experiments} completed: {result['config']}\")\n",
    "                        print(f\"  Best IoU: {result['best_val_iou']:.4f}\")\n",
    "                        print(f\"  Best Dice: {result['final_val_dice']:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"Experiment {experiment_count}/{total_experiments} failed or skipped\")\n",
    "                    \n",
    "                    print(\"-\" * 60)\n",
    "    \n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv('almgsi_quick_test_results.csv', index=False)\n",
    "        \n",
    "        print(f\"Al-Mg-Si-XCT quick test completed!\")\n",
    "        print(f\"Total successful experiments: {len(results)}\")\n",
    "        print(f\"Results saved to: almgsi_quick_test_results.csv\")\n",
    "        \n",
    "        # Quick analysis\n",
    "        print(\"\\nQUICK RESULTS ANALYSIS:\")\n",
    "        print(\"=\"*40)\n",
    "        best_idx = results_df['best_val_iou'].idxmax()\n",
    "        best_config = results_df.loc[best_idx]\n",
    "        print(f\"Best configuration: {best_config['config']}\")\n",
    "        print(f\"Best IoU: {best_config['best_val_iou']:.4f}\")\n",
    "        print(f\"Best Dice: {best_config['final_val_dice']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nAll Results:\")\n",
    "        for _, row in results_df.iterrows():\n",
    "            print(f\"  {row['config']}: IoU={row['best_val_iou']:.4f}, Dice={row['final_val_dice']:.4f}\")\n",
    "        \n",
    "        return results_df, histories\n",
    "    else:\n",
    "        print(\"No successful experiments completed\")\n",
    "        return pd.DataFrame(), {}\n",
    "\n",
    "def analyze_class_imbalance():\n",
    "    \"\"\"Analyze the class imbalance in the Al-Mg-Si-XCT dataset\"\"\"\n",
    "    print(\"ANALYZING CLASS IMBALANCE IN AL-MG-SI-XCT DATASET\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    image_paths, mask_paths = load_dataset_paths(\".\", \"Al-Mg-Si-XCT\")\n",
    "    \n",
    "    if len(image_paths) == 0:\n",
    "        print(\"No Al-Mg-Si-XCT data found\")\n",
    "        return\n",
    "    \n",
    "    total_pixels = 0\n",
    "    total_positive_pixels = 0\n",
    "    imbalance_ratios = []\n",
    "    \n",
    "    print(f\"Analyzing {len(mask_paths)} masks...\")\n",
    "    \n",
    "    for i, mask_path in enumerate(mask_paths[:min(10, len(mask_paths))]):  # Analyze first 10 masks\n",
    "        if mask_path.endswith('.tif') or mask_path.endswith('.tiff'):\n",
    "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if mask.dtype == np.uint16:\n",
    "                mask = (mask / 65535.0 * 255.0).astype(np.uint8)\n",
    "        else:\n",
    "            mask = np.array(Image.open(mask_path).convert('L'))\n",
    "        \n",
    "        mask = cv2.resize(mask, (256, 256), interpolation=cv2.INTER_NEAREST)\n",
    "        mask_binary = (mask > 127).astype(np.uint8)\n",
    "        \n",
    "        pixels = mask_binary.size\n",
    "        positive_pixels = np.sum(mask_binary)\n",
    "        negative_pixels = pixels - positive_pixels\n",
    "        \n",
    "        total_pixels += pixels\n",
    "        total_positive_pixels += positive_pixels\n",
    "        \n",
    "        if positive_pixels > 0:\n",
    "            ratio = negative_pixels / positive_pixels\n",
    "            imbalance_ratios.append(ratio)\n",
    "            print(f\"Mask {i+1}: {positive_pixels:6d}/{pixels:6d} positive ({positive_pixels/pixels*100:5.2f}%) - Ratio: {ratio:6.1f}:1\")\n",
    "        else:\n",
    "            print(f\"Mask {i+1}: {positive_pixels:6d}/{pixels:6d} positive ({positive_pixels/pixels*100:5.2f}%) - Ratio: ∞:1 (no positive pixels)\")\n",
    "    \n",
    "    overall_ratio = (total_pixels - total_positive_pixels) / max(total_positive_pixels, 1)\n",
    "    avg_ratio = np.mean(imbalance_ratios) if imbalance_ratios else float('inf')\n",
    "    \n",
    "    print(f\"\\nOVERALL STATISTICS:\")\n",
    "    print(f\"Total pixels analyzed: {total_pixels:,}\")\n",
    "    print(f\"Total positive pixels: {total_positive_pixels:,} ({total_positive_pixels/total_pixels*100:.3f}%)\")\n",
    "    print(f\"Overall imbalance ratio: {overall_ratio:.1f}:1\")\n",
    "    print(f\"Average imbalance ratio: {avg_ratio:.1f}:1\")\n",
    "    \n",
    "    print(f\"\\nRECOMMENDED LOSS FUNCTION SETTINGS:\")\n",
    "    print(f\"- Focal Loss gamma: 4.0-5.0 (high focus on hard examples)\")\n",
    "    print(f\"- Focal Loss alpha: 0.9-0.95 (high weight for positive class)\")\n",
    "    print(f\"- BCE pos_weight: {min(overall_ratio, 1000):.0f} (capped at 1000)\")\n",
    "    print(f\"- Tversky Loss beta (FN weight): 0.7-0.8 (high recall focus)\")\n",
    "\n",
    "def inspect_almgsi_data():\n",
    "    print(\"AL-MG-SI-XCT DATA INSPECTION\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    image_paths, mask_paths = load_dataset_paths(\".\", \"Al-Mg-Si-XCT\")\n",
    "    \n",
    "    if len(image_paths) > 0:\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        fig.suptitle('Al-Mg-Si-XCT Sample Data with Improved Contrast', fontsize=16)\n",
    "        \n",
    "        # Show 2 different samples\n",
    "        for row in range(2):\n",
    "            sample_idx = row\n",
    "            if sample_idx >= len(image_paths):\n",
    "                sample_idx = 0\n",
    "            \n",
    "            # Load and process image with proper bit depth handling\n",
    "            if image_paths[sample_idx].endswith('.tif'):\n",
    "                image = cv2.imread(image_paths[sample_idx], cv2.IMREAD_UNCHANGED)\n",
    "                print(f\"Sample {row+1} - Original image dtype: {image.dtype}, shape: {image.shape}\")\n",
    "                print(f\"Image value range: {image.min()} - {image.max()}\")\n",
    "                \n",
    "                if len(image.shape) == 3:\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                elif len(image.shape) == 2:\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "                \n",
    "                # Use percentile-based normalization for better contrast\n",
    "                if image.dtype == np.uint16:\n",
    "                    p1, p99 = np.percentile(image, (1, 99))\n",
    "                    image_display = np.clip((image - p1) / (p99 - p1) * 255, 0, 255).astype(np.uint8)\n",
    "                    print(f\"Applied percentile normalization: p1={p1}, p99={p99}\")\n",
    "                else:\n",
    "                    image_display = image.astype(np.uint8)\n",
    "            else:\n",
    "                image_display = np.array(Image.open(image_paths[sample_idx]).convert('RGB'))\n",
    "            \n",
    "            # Load and process mask\n",
    "            if mask_paths[sample_idx].endswith('.tif'):\n",
    "                mask = cv2.imread(mask_paths[sample_idx], cv2.IMREAD_GRAYSCALE)\n",
    "                print(f\"Sample {row+1} - Original mask dtype: {mask.dtype}, shape: {mask.shape}\")\n",
    "                print(f\"Mask value range: {mask.min()} - {mask.max()}\")\n",
    "                print(f\"Mask unique values: {np.unique(mask)}\")\n",
    "                \n",
    "                if mask.dtype == np.uint16:\n",
    "                    mask_display = (mask / 65535.0 * 255.0).astype(np.uint8)\n",
    "                else:\n",
    "                    mask_display = mask.astype(np.uint8)\n",
    "            else:\n",
    "                mask_display = np.array(Image.open(mask_paths[sample_idx]).convert('L'))\n",
    "            \n",
    "            # Calculate mask statistics\n",
    "            mask_binary = (mask_display > 127).astype(np.uint8)\n",
    "            total_pixels = mask_binary.size\n",
    "            positive_pixels = np.sum(mask_binary)\n",
    "            negative_pixels = total_pixels - positive_pixels\n",
    "            imbalance_ratio = negative_pixels / max(positive_pixels, 1)\n",
    "            \n",
    "            print(f\"Sample {row+1} - Mask statistics:\")\n",
    "            print(f\"  Total pixels: {total_pixels}\")\n",
    "            print(f\"  Positive pixels: {positive_pixels} ({positive_pixels/total_pixels*100:.2f}%)\")\n",
    "            print(f\"  Negative pixels: {negative_pixels} ({negative_pixels/total_pixels*100:.2f}%)\")\n",
    "            print(f\"  Imbalance ratio: {imbalance_ratio:.1f}:1\")\n",
    "            \n",
    "            # Display images\n",
    "            axes[row, 0].imshow(image_display)\n",
    "            axes[row, 0].set_title(f'Sample {row+1} - Original\\n(Percentile normalized)')\n",
    "            axes[row, 0].axis('off')\n",
    "            \n",
    "            axes[row, 1].imshow(mask_display, cmap='gray')\n",
    "            axes[row, 1].set_title(f'Sample {row+1} - Mask\\n({positive_pixels/total_pixels*100:.3f}% positive)')\n",
    "            axes[row, 1].axis('off')\n",
    "            \n",
    "            # Create better overlay\n",
    "            overlay = image_display.copy().astype(np.float32) / 255.0\n",
    "            # Highlight positive regions in red\n",
    "            overlay[:, :, 0] = np.where(mask_binary > 0, 1.0, overlay[:, :, 0])\n",
    "            overlay[:, :, 1] = np.where(mask_binary > 0, 0.0, overlay[:, :, 1])\n",
    "            overlay[:, :, 2] = np.where(mask_binary > 0, 0.0, overlay[:, :, 2])\n",
    "            \n",
    "            axes[row, 2].imshow(np.clip(overlay, 0, 1))\n",
    "            axes[row, 2].set_title(f'Sample {row+1} - Overlay\\nRatio: {imbalance_ratio:.1f}:1')\n",
    "            axes[row, 2].axis('off')\n",
    "        \n",
    "        print(f\"Found {len(image_paths)} Al-Mg-Si-XCT image-mask pairs\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('almgsi_improved_sample_data.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No Al-Mg-Si-XCT data found\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU Memory available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(\"Starting Al-Mg-Si-XCT class imbalance analysis...\")\n",
    "analyze_class_imbalance()\n",
    "\n",
    "print(\"\\nStarting Al-Mg-Si-XCT data inspection...\")\n",
    "inspect_almgsi_data()\n",
    "\n",
    "print(\"\\nStarting Al-Mg-Si-XCT quick test with improved loss functions...\")\n",
    "almgsi_results, almgsi_histories = run_almgsi_quick_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6811b3e-ca74-4fea-81b4-a586d3b5be68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.2+cu118\n",
      "CUDA available: True\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Device name: Tesla P100-PCIE-12GB\n",
      "GPU Memory available: 11.9 GB\n",
      "Starting patch-based MDSMG experiment...\n",
      "MDSMG: Material Data Science Model Garden\n",
      "Patch-Based Al-Mg-Si-XCT Dataset Experiment\n",
      "================================================================================\n",
      "Starting patch-based Al-Mg-Si-XCT experiment...\n",
      "Starting MDSMG Patch-Based Al-Mg-Si-XCT Experiment on cuda\n",
      "Total experiments to run: 12\n",
      "Running experiment 1/12: resnet50_unet_imagenet\n",
      "PATCH EXPERIMENT: Al-Mg-Si-XCT | resnet50 | unet | imagenet\n",
      "================================================================================\n",
      "Loading dataset: Al-Mg-Si-XCT\n",
      "Image directory: ./Al-Mg-Si-XCT/images\n",
      "Mask directory: ./Al-Mg-Si-XCT/masks\n",
      "Found 150 valid image-mask pairs\n",
      "Dataset split: 120 train, 30 validation\n",
      "Extracting training patches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:42<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 10579 foreground patches\n",
      "Extracted 162701 background patches\n",
      "Extracting validation patches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: 4248 fg patches, 6000 bg patches\n",
      "Total patches: 161685\n",
      "Foreground ratio: 0.981\n",
      "Creating encoder: resnet50 with imagenet pretraining\n",
      "✓ ResNet50 with ImageNet weights loaded\n",
      "Training resnet50_unet_imagenet for 30 epochs on cuda\n",
      "Using Combined Patch Loss (Focal + Dice)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██████████| 10106/10106 [16:28<00:00, 10.23it/s, train_loss=0.1414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: Train Loss: 0.1516 | Val Loss: 0.1442 | Val IoU: 0.7914 | Val Dice: 0.8296\n",
      "  ✓ New best IoU: 0.7914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|██████████| 10106/10106 [16:23<00:00, 10.27it/s, train_loss=0.1036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2: Train Loss: 0.1295 | Val Loss: 0.1346 | Val IoU: 0.8074 | Val Dice: 0.8455\n",
      "  ✓ New best IoU: 0.8074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|██████████| 10106/10106 [16:28<00:00, 10.23it/s, train_loss=0.0878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3: Train Loss: 0.1232 | Val Loss: 0.1287 | Val IoU: 0.8225 | Val Dice: 0.8651\n",
      "  ✓ New best IoU: 0.8225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|██████████| 10106/10106 [16:27<00:00, 10.24it/s, train_loss=0.1168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4: Train Loss: 0.1192 | Val Loss: 0.1212 | Val IoU: 0.8270 | Val Dice: 0.8667\n",
      "  ✓ New best IoU: 0.8270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|██████████| 10106/10106 [16:25<00:00, 10.26it/s, train_loss=0.1180]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5: Train Loss: 0.1158 | Val Loss: 0.1148 | Val IoU: 0.8378 | Val Dice: 0.8768\n",
      "  ✓ New best IoU: 0.8378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|██████████| 10106/10106 [16:29<00:00, 10.21it/s, train_loss=0.0991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6: Train Loss: 0.1132 | Val Loss: 0.1142 | Val IoU: 0.8385 | Val Dice: 0.8782\n",
      "  ✓ New best IoU: 0.8385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|██████████| 10106/10106 [16:20<00:00, 10.31it/s, train_loss=0.0907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7: Train Loss: 0.1109 | Val Loss: 0.1118 | Val IoU: 0.8454 | Val Dice: 0.8867\n",
      "  ✓ New best IoU: 0.8454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|██████████| 10106/10106 [16:00<00:00, 10.52it/s, train_loss=0.1296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8: Train Loss: 0.1089 | Val Loss: 0.1151 | Val IoU: 0.8428 | Val Dice: 0.8850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|██████████| 10106/10106 [16:07<00:00, 10.45it/s, train_loss=0.2150]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9: Train Loss: 0.1072 | Val Loss: 0.1093 | Val IoU: 0.8452 | Val Dice: 0.8860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 100%|██████████| 10106/10106 [16:03<00:00, 10.48it/s, train_loss=0.1348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.1066 | Val Loss: 0.1094 | Val IoU: 0.8486 | Val Dice: 0.8896\n",
      "  ✓ New best IoU: 0.8486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: 100%|██████████| 10106/10106 [16:03<00:00, 10.48it/s, train_loss=0.1189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 0.1143 | Val Loss: 0.1195 | Val IoU: 0.8351 | Val Dice: 0.8777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 100%|██████████| 10106/10106 [16:03<00:00, 10.49it/s, train_loss=0.1135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss: 0.1132 | Val Loss: 0.1141 | Val IoU: 0.8445 | Val Dice: 0.8866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30: 100%|██████████| 10106/10106 [16:04<00:00, 10.48it/s, train_loss=0.1425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss: 0.1117 | Val Loss: 0.1140 | Val IoU: 0.8423 | Val Dice: 0.8841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30: 100%|██████████| 10106/10106 [16:00<00:00, 10.52it/s, train_loss=0.0849]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss: 0.1103 | Val Loss: 0.1198 | Val IoU: 0.8405 | Val Dice: 0.8838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30: 100%|██████████| 10106/10106 [16:03<00:00, 10.49it/s, train_loss=0.1616]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss: 0.1091 | Val Loss: 0.1121 | Val IoU: 0.8451 | Val Dice: 0.8865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30: 100%|██████████| 10106/10106 [16:05<00:00, 10.47it/s, train_loss=0.1330]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss: 0.1076 | Val Loss: 0.1090 | Val IoU: 0.8487 | Val Dice: 0.8887\n",
      "  ✓ New best IoU: 0.8487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30: 100%|██████████| 10106/10106 [16:04<00:00, 10.48it/s, train_loss=0.1388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss: 0.1062 | Val Loss: 0.1141 | Val IoU: 0.8460 | Val Dice: 0.8887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30: 100%|██████████| 10106/10106 [16:05<00:00, 10.46it/s, train_loss=0.1348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss: 0.1047 | Val Loss: 0.1179 | Val IoU: 0.8418 | Val Dice: 0.8839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30: 100%|██████████| 10106/10106 [16:01<00:00, 10.51it/s, train_loss=0.0927]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss: 0.1039 | Val Loss: 0.1082 | Val IoU: 0.8487 | Val Dice: 0.8887\n",
      "  ✓ New best IoU: 0.8487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30: 100%|██████████| 10106/10106 [16:06<00:00, 10.46it/s, train_loss=0.1018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss: 0.1024 | Val Loss: 0.1087 | Val IoU: 0.8507 | Val Dice: 0.8908\n",
      "  ✓ New best IoU: 0.8507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30: 100%|██████████| 10106/10106 [16:17<00:00, 10.34it/s, train_loss=0.0931]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss: 0.1014 | Val Loss: 0.1126 | Val IoU: 0.8462 | Val Dice: 0.8886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30: 100%|██████████| 10106/10106 [16:29<00:00, 10.21it/s, train_loss=0.0626]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Loss: 0.1002 | Val Loss: 0.1071 | Val IoU: 0.8502 | Val Dice: 0.8901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30: 100%|██████████| 10106/10106 [16:18<00:00, 10.33it/s, train_loss=0.1540]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Loss: 0.0990 | Val Loss: 0.1071 | Val IoU: 0.8502 | Val Dice: 0.8900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30: 100%|██████████| 10106/10106 [16:04<00:00, 10.47it/s, train_loss=0.1183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train Loss: 0.0973 | Val Loss: 0.1076 | Val IoU: 0.8508 | Val Dice: 0.8920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30: 100%|██████████| 10106/10106 [16:03<00:00, 10.49it/s, train_loss=0.0973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Train Loss: 0.0966 | Val Loss: 0.1069 | Val IoU: 0.8511 | Val Dice: 0.8922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30: 100%|██████████| 10106/10106 [16:04<00:00, 10.48it/s, train_loss=0.0752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train Loss: 0.0960 | Val Loss: 0.1068 | Val IoU: 0.8511 | Val Dice: 0.8923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30:  94%|█████████▍| 9479/10106 [15:02<00:58, 10.64it/s, train_loss=0.0934]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 1/30:  96%|█████████▌| 9683/10106 [15:20<00:40, 10.39it/s, train_loss=0.1401]]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 2/30: 100%|██████████| 10106/10106 [16:00<00:00, 10.52it/s, train_loss=0.0799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2: Train Loss: 0.1346 | Val Loss: 0.1380 | Val IoU: 0.7718 | Val Dice: 0.8107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30:  94%|█████████▍| 9487/10106 [15:15<00:58, 10.57it/s, train_loss=0.1162]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4: Train Loss: 0.1225 | Val Loss: 0.1270 | Val IoU: 0.8265 | Val Dice: 0.8681\n",
      "  ✓ New best IoU: 0.8265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30:  67%|██████▋   | 6732/10106 [10:51<05:24, 10.41it/s, train_loss=0.1161]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy import stats\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "import segmentation_models_pytorch as smp\n",
    "from huggingface_hub import hf_hub_download\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current device:\", torch.cuda.current_device())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name())\n",
    "\n",
    "def extract_smart_patches(image_path, mask_path, patch_size=128, stride=64, min_foreground_ratio=0.001):\n",
    "    try:\n",
    "        if image_path.endswith('.tif') or image_path.endswith('.tiff'):\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "            if len(image.shape) == 3:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            elif len(image.shape) == 2:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "            \n",
    "            if image.dtype == np.uint16:\n",
    "                p1, p99 = np.percentile(image, (1, 99))\n",
    "                image = np.clip((image - p1) / (p99 - p1) * 255, 0, 255).astype(np.uint8)\n",
    "            elif image.dtype != np.uint8:\n",
    "                image = image.astype(np.uint8)\n",
    "        else:\n",
    "            image = np.array(Image.open(image_path).convert('RGB'))\n",
    "            \n",
    "        if mask_path.endswith('.tif') or mask_path.endswith('.tiff'):\n",
    "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if mask.dtype == np.uint16:\n",
    "                mask_max = mask.max()\n",
    "                if mask_max > 255:\n",
    "                    mask = (mask.astype(np.float32) / mask_max * 255.0).astype(np.uint8)\n",
    "                else:\n",
    "                    mask = mask.astype(np.uint8)\n",
    "            elif mask.dtype != np.uint8:\n",
    "                mask = mask.astype(np.uint8)\n",
    "        else:\n",
    "            mask = np.array(Image.open(mask_path).convert('L'))\n",
    "        \n",
    "        mask = (mask > 127).astype(np.uint8)\n",
    "        \n",
    "        h, w = mask.shape[:2]\n",
    "        foreground_patches = []\n",
    "        background_patches = []\n",
    "        \n",
    "        for y in range(0, h - patch_size + 1, stride):\n",
    "            for x in range(0, w - patch_size + 1, stride):\n",
    "                img_patch = image[y:y+patch_size, x:x+patch_size]\n",
    "                mask_patch = mask[y:y+patch_size, x:x+patch_size]\n",
    "                \n",
    "                foreground_ratio = np.mean(mask_patch)\n",
    "                \n",
    "                if foreground_ratio > min_foreground_ratio:\n",
    "                    foreground_patches.append((img_patch, mask_patch))\n",
    "                else:\n",
    "                    background_patches.append((img_patch, mask_patch))\n",
    "        \n",
    "        return foreground_patches, background_patches\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting patches from {image_path}: {e}\")\n",
    "        return [], []\n",
    "\n",
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, foreground_patches, background_patches, fg_oversample_ratio=15, \n",
    "                 max_bg_patches=3000, patch_size=128, transform=None):\n",
    "        self.patch_size = patch_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        if len(background_patches) > max_bg_patches:\n",
    "            background_patches = random.sample(background_patches, max_bg_patches)\n",
    "        \n",
    "        self.patches = []\n",
    "        \n",
    "        for img_patch, mask_patch in background_patches:\n",
    "            self.patches.append((img_patch, mask_patch, False))\n",
    "        \n",
    "        for _ in range(fg_oversample_ratio):\n",
    "            for img_patch, mask_patch in foreground_patches:\n",
    "                self.patches.append((img_patch, mask_patch, True))\n",
    "        \n",
    "        print(f\"Total patches: {len(self.patches)}\")\n",
    "        print(f\"Foreground ratio: {len(foreground_patches) * fg_oversample_ratio / len(self.patches):.3f}\")\n",
    "        \n",
    "        random.shuffle(self.patches)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_patch, mask_patch, need_strong_aug = self.patches[idx]\n",
    "        \n",
    "        img_patch = cv2.resize(img_patch, (self.patch_size, self.patch_size))\n",
    "        mask_patch = cv2.resize(mask_patch, (self.patch_size, self.patch_size), \n",
    "                              interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        if self.transform:\n",
    "            if need_strong_aug:\n",
    "                strong_transform = A.Compose([\n",
    "                    A.HorizontalFlip(p=0.5),\n",
    "                    A.VerticalFlip(p=0.5),\n",
    "                    A.RandomRotate90(p=0.5),\n",
    "                    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "                    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "                    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "                    A.GaussianBlur(blur_limit=3, p=0.2),\n",
    "                    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    ToTensorV2(),\n",
    "                ])\n",
    "                augmented = strong_transform(image=img_patch, mask=mask_patch)\n",
    "            else:\n",
    "                augmented = self.transform(image=img_patch, mask=mask_patch)\n",
    "            \n",
    "            img_patch = augmented['image']\n",
    "            mask_patch = augmented['mask']\n",
    "        else:\n",
    "            img_patch = img_patch.astype(np.float32)\n",
    "            img_patch = torch.from_numpy(img_patch.transpose(2, 0, 1)).float() / 255.0\n",
    "            mask_patch = torch.from_numpy(mask_patch.astype(np.float32)).float()\n",
    "            \n",
    "        return img_patch, mask_patch\n",
    "\n",
    "def load_dataset_paths(base_path, dataset_name):\n",
    "    dataset_path = os.path.join(base_path, dataset_name)\n",
    "    image_dir = os.path.join(dataset_path, 'images')\n",
    "    mask_dir = os.path.join(dataset_path, 'masks')\n",
    "    \n",
    "    print(f\"Loading dataset: {dataset_name}\")\n",
    "    print(f\"Image directory: {image_dir}\")\n",
    "    print(f\"Mask directory: {mask_dir}\")\n",
    "    \n",
    "    if not os.path.exists(image_dir) or not os.path.exists(mask_dir):\n",
    "        print(f\"Warning: Directory does not exist for {dataset_name}\")\n",
    "        return [], []\n",
    "    \n",
    "    image_extensions = ['*.png', '*.tif', '*.tiff', '*.jpg', '*.jpeg']\n",
    "    image_paths = []\n",
    "    for ext in image_extensions:\n",
    "        image_paths.extend(glob.glob(os.path.join(image_dir, ext)))\n",
    "    \n",
    "    mask_paths = []\n",
    "    for img_path in image_paths:\n",
    "        img_name = os.path.basename(img_path)\n",
    "        img_name_no_ext = os.path.splitext(img_name)[0]\n",
    "        \n",
    "        for mask_ext in ['png', 'tif', 'tiff']:\n",
    "            mask_path = os.path.join(mask_dir, f\"{img_name_no_ext}.{mask_ext}\")\n",
    "            if os.path.exists(mask_path):\n",
    "                mask_paths.append(mask_path)\n",
    "                break\n",
    "    \n",
    "    valid_pairs = []\n",
    "    valid_mask_paths = []\n",
    "    for img_path, mask_path in zip(image_paths, mask_paths):\n",
    "        if os.path.exists(img_path) and os.path.exists(mask_path):\n",
    "            valid_pairs.append(img_path)\n",
    "            valid_mask_paths.append(mask_path)\n",
    "    \n",
    "    print(f\"Found {len(valid_pairs)} valid image-mask pairs\")\n",
    "    return valid_pairs, valid_mask_paths\n",
    "\n",
    "def load_micronet_weights(model_name='micronet-resnet50'):\n",
    "    try:\n",
    "        if model_name == 'micronet-resnet50':\n",
    "            filenames_to_try = [\n",
    "                \"resnet50_micronet_weights.pth\",\n",
    "                \"resnet50_imagenet-micronet_weights.pth\", \n",
    "                \"pytorch_model.bin\"\n",
    "            ]\n",
    "            repo_id = \"jstuckner/microscopy-resnet50-micronet\"\n",
    "            \n",
    "        elif model_name == 'micronet-se_resnext101':\n",
    "            filenames_to_try = [\n",
    "                \"resnext101_micronet_weights.pth\",\n",
    "                \"resnext101_imagenet-micronet_weights.pth\",\n",
    "                \"pytorch_model.bin\"\n",
    "            ]\n",
    "            repo_id = \"jstuckner/microscopy-resnext101-micronet\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown MicroNet model: {model_name}\")\n",
    "        \n",
    "        weights = None\n",
    "        for filename in filenames_to_try:\n",
    "            try:\n",
    "                model_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "                weights = torch.load(model_path, map_location='cpu')\n",
    "                print(f\"Successfully loaded MicroNet weights for {model_name} from {filename}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {filename}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if weights is None:\n",
    "            print(f\"Failed to load any weights for {model_name}\")\n",
    "            \n",
    "        return weights\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load MicroNet weights for {model_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.squeeze(x).view(b, c)\n",
    "        y = self.excitation(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "def create_encoder(encoder_name='resnet50', pretrained='imagenet'):\n",
    "    print(f\"Creating encoder: {encoder_name} with {pretrained} pretraining\")\n",
    "    \n",
    "    if encoder_name == 'resnet50':\n",
    "        if pretrained == 'imagenet':\n",
    "            encoder = smp.encoders.get_encoder('resnet50', in_channels=3, weights='imagenet')\n",
    "            print(\"✓ ResNet50 with ImageNet weights loaded\")\n",
    "        elif pretrained == 'micronet':\n",
    "            encoder = smp.encoders.get_encoder('resnet50', in_channels=3, weights=None)\n",
    "            micronet_weights = load_micronet_weights('micronet-resnet50')\n",
    "            if micronet_weights:\n",
    "                try:\n",
    "                    encoder.load_state_dict(micronet_weights, strict=False)\n",
    "                    print(\"✓ ResNet50 with MicroNet weights loaded\")\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠ Failed to load MicroNet weights strictly, trying partial loading: {str(e)}\")\n",
    "                    model_dict = encoder.state_dict()\n",
    "                    pretrained_dict = {k: v for k, v in micronet_weights.items() \n",
    "                                     if k in model_dict and v.shape == model_dict[k].shape}\n",
    "                    model_dict.update(pretrained_dict)\n",
    "                    encoder.load_state_dict(model_dict)\n",
    "                    print(f\"✓ ResNet50 with partial MicroNet weights loaded ({len(pretrained_dict)} layers)\")\n",
    "            else:\n",
    "                print(\"⚠ Failed to load MicroNet weights, using random initialization\")\n",
    "        else:\n",
    "            encoder = smp.encoders.get_encoder('resnet50', in_channels=3, weights=None)\n",
    "            print(\"✓ ResNet50 with random weights\")\n",
    "            \n",
    "        encoder_channels = [3, 64, 256, 512, 1024, 2048]\n",
    "    \n",
    "    elif encoder_name == 'se_resnext101':\n",
    "        if pretrained == 'imagenet':\n",
    "            encoder = smp.encoders.get_encoder('se_resnext101_32x4d', in_channels=3, weights='imagenet')\n",
    "            print(\"✓ SE-ResNeXt101 with ImageNet weights loaded\")\n",
    "        elif pretrained == 'micronet':\n",
    "            print(\"⚠ MicroNet weights for SE-ResNeXt101 have architecture mismatch\")\n",
    "            print(\"  Using ImageNet pretrained weights as fallback\")\n",
    "            encoder = smp.encoders.get_encoder('se_resnext101_32x4d', in_channels=3, weights='imagenet')\n",
    "            \n",
    "            micronet_weights = load_micronet_weights('micronet-se_resnext101')\n",
    "            if micronet_weights:\n",
    "                try:\n",
    "                    model_dict = encoder.state_dict()\n",
    "                    pretrained_dict = {}\n",
    "                    for k, v in micronet_weights.items():\n",
    "                        if k in model_dict and v.shape == model_dict[k].shape:\n",
    "                            pretrained_dict[k] = v\n",
    "                    \n",
    "                    if len(pretrained_dict) > 0:\n",
    "                        model_dict.update(pretrained_dict)\n",
    "                        encoder.load_state_dict(model_dict)\n",
    "                        print(f\"✓ Loaded {len(pretrained_dict)} compatible layers from MicroNet\")\n",
    "                    else:\n",
    "                        print(\"✗ No compatible layers found in MicroNet weights\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"✗ Failed to load any MicroNet weights: {str(e)}\")\n",
    "            \n",
    "        else:\n",
    "            encoder = smp.encoders.get_encoder('se_resnext101_32x4d', in_channels=3, weights=None)\n",
    "            print(\"✓ SE-ResNeXt101 with random weights\")\n",
    "            \n",
    "        encoder_channels = [3, 64, 256, 512, 1024, 2048]\n",
    "    \n",
    "    return encoder, encoder_channels\n",
    "\n",
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, encoder_channels, decoder_channels, n_classes=1):\n",
    "        super(UNetDecoder, self).__init__()\n",
    "        \n",
    "        self.decoder_blocks = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(decoder_channels)):\n",
    "            if i == 0:\n",
    "                in_ch = encoder_channels[-1]\n",
    "            else:\n",
    "                in_ch = decoder_channels[i-1]\n",
    "            \n",
    "            out_ch = decoder_channels[i]\n",
    "            skip_ch = encoder_channels[-(i+2)] if i < len(encoder_channels)-1 else 0\n",
    "            \n",
    "            block = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2),\n",
    "                nn.Conv2d(out_ch + skip_ch, out_ch, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "            \n",
    "            self.decoder_blocks.append(block)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(decoder_channels[-1], n_classes, kernel_size=1)\n",
    "        \n",
    "    def forward(self, encoder_features):\n",
    "        x = encoder_features[-1]\n",
    "        \n",
    "        for i, decoder_block in enumerate(self.decoder_blocks):\n",
    "            x = decoder_block[0](x)\n",
    "            \n",
    "            if i < len(encoder_features) - 1:\n",
    "                skip = encoder_features[-(i+2)]\n",
    "                if x.shape[2:] != skip.shape[2:]:\n",
    "                    x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
    "                x = torch.cat([x, skip], dim=1)\n",
    "            \n",
    "            for layer in decoder_block[1:]:\n",
    "                x = layer(x)\n",
    "        \n",
    "        return self.final_conv(x)\n",
    "\n",
    "class ASPPModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, atrous_rates):\n",
    "        super(ASPPModule, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.atrous_convs = nn.ModuleList()\n",
    "        for rate in atrous_rates:\n",
    "            self.atrous_convs.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, padding=rate, dilation=rate, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ))\n",
    "        \n",
    "        self.global_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(out_channels * (2 + len(atrous_rates)), out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[2:]\n",
    "        \n",
    "        feat1 = self.conv1(x)\n",
    "        atrous_feats = [conv(x) for conv in self.atrous_convs]\n",
    "        global_feat = self.global_pool(x)\n",
    "        global_feat = F.interpolate(global_feat, size=(h, w), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        concat_feat = torch.cat([feat1] + atrous_feats + [global_feat], dim=1)\n",
    "        return self.project(concat_feat)\n",
    "\n",
    "class DeepLabV3Decoder(nn.Module):\n",
    "    def __init__(self, encoder_channels, n_classes=1):\n",
    "        super(DeepLabV3Decoder, self).__init__()\n",
    "        self.aspp = ASPPModule(encoder_channels[-1], 256, [6, 12, 18])\n",
    "        self.classifier = nn.Conv2d(256, n_classes, 1)\n",
    "        \n",
    "    def forward(self, encoder_features):\n",
    "        x = encoder_features[-1]\n",
    "        x = self.aspp(x)\n",
    "        x = self.classifier(x)\n",
    "        x = F.interpolate(x, scale_factor=32, mode='bilinear', align_corners=False)\n",
    "        return x\n",
    "\n",
    "class DeepLabV3PlusDecoder(nn.Module):\n",
    "    def __init__(self, encoder_channels, n_classes=1):\n",
    "        super(DeepLabV3PlusDecoder, self).__init__()\n",
    "        \n",
    "        self.aspp = ASPPModule(encoder_channels[-1], 256, [6, 12, 18])\n",
    "        low_level_channels = encoder_channels[1]\n",
    "        \n",
    "        self.low_level_proj = nn.Sequential(\n",
    "            nn.Conv2d(low_level_channels, 48, 1, bias=False),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(256 + 48, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Conv2d(256, n_classes, 1)\n",
    "        \n",
    "    def forward(self, encoder_features):\n",
    "        low_level = encoder_features[1]\n",
    "        high_level = encoder_features[-1]\n",
    "        \n",
    "        high_level = self.aspp(high_level)\n",
    "        high_level = F.interpolate(high_level, size=low_level.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        low_level = self.low_level_proj(low_level)\n",
    "        \n",
    "        concat_feat = torch.cat([high_level, low_level], dim=1)\n",
    "        decoded = self.decoder(concat_feat)\n",
    "        output = self.classifier(decoded)\n",
    "        \n",
    "        output = F.interpolate(output, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class MaterialSegmentationModel(nn.Module):\n",
    "    def __init__(self, encoder_name='resnet50', decoder_name='unet', pretrained='imagenet', n_classes=1):\n",
    "        super(MaterialSegmentationModel, self).__init__()\n",
    "        \n",
    "        self.encoder, encoder_channels = create_encoder(encoder_name, pretrained)\n",
    "        \n",
    "        if decoder_name == 'unet':\n",
    "            self.decoder = UNetDecoder(encoder_channels[1:], [256, 128, 64, 32], n_classes)\n",
    "        elif decoder_name == 'deeplabv3':\n",
    "            self.decoder = DeepLabV3Decoder(encoder_channels[1:], n_classes)\n",
    "        elif decoder_name == 'deeplabv3plus':\n",
    "            self.decoder = DeepLabV3PlusDecoder(encoder_channels, n_classes)\n",
    "        \n",
    "        self.config = f\"{encoder_name}_{decoder_name}_{pretrained}\"\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoder_features = self.encoder(x)\n",
    "        \n",
    "        if hasattr(self.decoder, '__class__') and self.decoder.__class__.__name__ == 'DeepLabV3PlusDecoder':\n",
    "            output = self.decoder(encoder_features)\n",
    "        else:\n",
    "            output = self.decoder(encoder_features[1:])\n",
    "        \n",
    "        return output\n",
    "\n",
    "class CombinedPatchLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.95, gamma=3.0, dice_weight=0.6, focal_weight=0.4):\n",
    "        super(CombinedPatchLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.dice_weight = dice_weight\n",
    "        self.focal_weight = focal_weight\n",
    "        \n",
    "    def focal_loss(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        \n",
    "        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "        focal_loss = alpha_t * (1 - pt) ** self.gamma * bce_loss\n",
    "        \n",
    "        return focal_loss.mean()\n",
    "    \n",
    "    def dice_loss(self, inputs, targets, smooth=1e-6):\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "        \n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n",
    "        \n",
    "        return 1 - dice\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        focal = self.focal_loss(inputs, targets)\n",
    "        dice = self.dice_loss(inputs, targets)\n",
    "        \n",
    "        return self.focal_weight * focal + self.dice_weight * dice\n",
    "\n",
    "def dice_coefficient(pred, target, smooth=1e-6):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred = (pred > 0.5).float()\n",
    "    \n",
    "    if target.dim() == 3 and pred.dim() == 4:\n",
    "        target = target.unsqueeze(1)\n",
    "    elif target.dim() == 4 and pred.dim() == 3:\n",
    "        pred = pred.unsqueeze(1)\n",
    "    \n",
    "    intersection = (pred * target).sum(dim=(2, 3))\n",
    "    union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
    "    \n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return dice.mean()\n",
    "\n",
    "def iou_score(pred, target, smooth=1e-6):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred = (pred > 0.5).float()\n",
    "    \n",
    "    if target.dim() == 3 and pred.dim() == 4:\n",
    "        target = target.unsqueeze(1)\n",
    "    elif target.dim() == 4 and pred.dim() == 3:\n",
    "        pred = pred.unsqueeze(1)\n",
    "    \n",
    "    intersection = (pred * target).sum(dim=(2, 3))\n",
    "    union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3)) - intersection\n",
    "    \n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return iou.mean()\n",
    "\n",
    "def train_patch_model(model, train_loader, val_loader, num_epochs=5, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    \n",
    "    criterion = CombinedPatchLoss(alpha=0.95, gamma=3.0, dice_weight=0.6, focal_weight=0.4)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    best_val_iou = 0\n",
    "    history = {\n",
    "        'train_losses': [],\n",
    "        'val_losses': [],\n",
    "        'val_ious': [],\n",
    "        'val_dices': []\n",
    "    }\n",
    "    \n",
    "    print(f\"Training {model.config} for {num_epochs} epochs on {device}\")\n",
    "    print(f\"Using Combined Patch Loss (Focal + Dice)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for images, masks in pbar:\n",
    "            images, masks = images.to(device), masks.to(device).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            if outputs.shape[2:] != masks.shape[1:]:\n",
    "                outputs = F.interpolate(outputs, size=masks.shape[1:], mode='bilinear', align_corners=False)\n",
    "            \n",
    "            outputs = outputs.squeeze(1)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "            \n",
    "            pbar.set_postfix({'train_loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            if train_batches % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_iou = 0\n",
    "        val_dice = 0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(device), masks.to(device).float()\n",
    "                \n",
    "                outputs = model(images)\n",
    "                if outputs.shape[2:] != masks.shape[1:]:\n",
    "                    outputs = F.interpolate(outputs, size=masks.shape[1:], mode='bilinear', align_corners=False)\n",
    "                \n",
    "                outputs = outputs.squeeze(1)\n",
    "                loss = criterion(outputs, masks)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                if masks.dim() == 3:\n",
    "                    masks_for_metrics = masks.unsqueeze(1)\n",
    "                else:\n",
    "                    masks_for_metrics = masks\n",
    "                \n",
    "                if outputs.dim() == 3:\n",
    "                    outputs_for_metrics = outputs.unsqueeze(1)\n",
    "                else:\n",
    "                    outputs_for_metrics = outputs\n",
    "                \n",
    "                val_iou += iou_score(outputs_for_metrics, masks_for_metrics).item()\n",
    "                val_dice += dice_coefficient(outputs_for_metrics, masks_for_metrics).item()\n",
    "                val_batches += 1\n",
    "        \n",
    "        avg_train_loss = train_loss / train_batches\n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        avg_val_iou = val_iou / val_batches\n",
    "        avg_val_dice = val_dice / val_batches\n",
    "        \n",
    "        history['train_losses'].append(avg_train_loss)\n",
    "        history['val_losses'].append(avg_val_loss)\n",
    "        history['val_ious'].append(avg_val_iou)\n",
    "        history['val_dices'].append(avg_val_dice)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1:2d}: Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val IoU: {avg_val_iou:.4f} | Val Dice: {avg_val_dice:.4f}')\n",
    "        \n",
    "        if avg_val_iou > best_val_iou:\n",
    "            best_val_iou = avg_val_iou\n",
    "            torch.save(model.state_dict(), f'best_patch_{model.config}.pth')\n",
    "            print(f'  ✓ New best IoU: {best_val_iou:.4f}')\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    history['best_val_iou'] = best_val_iou\n",
    "    print(f\"Training completed! Best validation IoU: {best_val_iou:.4f}\")\n",
    "    print(f\"Final validation Dice: {history['val_dices'][-1]:.4f}\")\n",
    "    return history\n",
    "\n",
    "def run_patch_experiment(dataset_name, encoder_name, decoder_name, pretrained, device='cuda'):\n",
    "    print(f\"PATCH EXPERIMENT: {dataset_name} | {encoder_name} | {decoder_name} | {pretrained}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    image_paths, mask_paths = load_dataset_paths(\".\", dataset_name)\n",
    "    \n",
    "    if len(image_paths) < 4:\n",
    "        print(f\"Insufficient data for {dataset_name}: {len(image_paths)} samples\")\n",
    "        return None, None\n",
    "    \n",
    "    train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        image_paths, mask_paths, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset split: {len(train_images)} train, {len(val_images)} validation\")\n",
    "    \n",
    "    print(\"Extracting training patches...\")\n",
    "    train_fg_patches = []\n",
    "    train_bg_patches = []\n",
    "    \n",
    "    for img_path, mask_path in tqdm(zip(train_images, train_masks), total=len(train_images)):\n",
    "        fg_patches, bg_patches = extract_smart_patches(\n",
    "            img_path, mask_path, \n",
    "            patch_size=128, \n",
    "            stride=64,\n",
    "            min_foreground_ratio=0.001\n",
    "        )\n",
    "        train_fg_patches.extend(fg_patches)\n",
    "        train_bg_patches.extend(bg_patches)\n",
    "    \n",
    "    print(f\"Extracted {len(train_fg_patches)} foreground patches\")\n",
    "    print(f\"Extracted {len(train_bg_patches)} background patches\")\n",
    "    \n",
    "    print(\"Extracting validation patches...\")\n",
    "    val_fg_patches = []\n",
    "    val_bg_patches = []\n",
    "    \n",
    "    for img_path, mask_path in tqdm(zip(val_images, val_masks), total=len(val_images)):\n",
    "        fg_patches, bg_patches = extract_smart_patches(\n",
    "            img_path, mask_path, \n",
    "            patch_size=128, \n",
    "            stride=64,\n",
    "            min_foreground_ratio=0.0\n",
    "        )\n",
    "        val_fg_patches.extend(fg_patches)\n",
    "        val_bg_patches.extend(bg_patches[:min(len(bg_patches), 200)])\n",
    "    \n",
    "    print(f\"Validation: {len(val_fg_patches)} fg patches, {len(val_bg_patches)} bg patches\")\n",
    "    \n",
    "    train_transform = A.Compose([\n",
    "        A.HorizontalFlip(p=0.3),\n",
    "        A.VerticalFlip(p=0.3),\n",
    "        A.RandomRotate90(p=0.3),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    val_transform = A.Compose([\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    train_dataset = PatchDataset(\n",
    "        train_fg_patches, train_bg_patches,\n",
    "        fg_oversample_ratio=15,\n",
    "        max_bg_patches=3000,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    val_patches = [(img, mask, False) for img, mask in val_fg_patches + val_bg_patches]\n",
    "    random.shuffle(val_patches)\n",
    "    \n",
    "    class SimpleValDataset(Dataset):\n",
    "        def __init__(self, patches, transform, patch_size=128):\n",
    "            self.patches = patches\n",
    "            self.transform = transform\n",
    "            self.patch_size = patch_size\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.patches)\n",
    "            \n",
    "        def __getitem__(self, idx):\n",
    "            img_patch, mask_patch, _ = self.patches[idx]\n",
    "            img_patch = cv2.resize(img_patch, (self.patch_size, self.patch_size))\n",
    "            mask_patch = cv2.resize(mask_patch, (self.patch_size, self.patch_size), \n",
    "                                  interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "            if self.transform:\n",
    "                augmented = self.transform(image=img_patch, mask=mask_patch)\n",
    "                return augmented['image'], augmented['mask']\n",
    "            else:\n",
    "                img_patch = torch.from_numpy(img_patch.transpose(2, 0, 1)).float() / 255.0\n",
    "                mask_patch = torch.from_numpy(mask_patch.astype(np.float32))\n",
    "                return img_patch, mask_patch\n",
    "    \n",
    "    val_dataset = SimpleValDataset(val_patches, val_transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "    \n",
    "    model = MaterialSegmentationModel(encoder_name, decoder_name, pretrained, n_classes=1)\n",
    "    \n",
    "    history = train_patch_model(model, train_loader, val_loader, num_epochs=30, device=device)\n",
    "    \n",
    "    result = {\n",
    "        'dataset': dataset_name,\n",
    "        'encoder': encoder_name,\n",
    "        'decoder': decoder_name,\n",
    "        'pretrained': pretrained,\n",
    "        'config': f\"{encoder_name}_{decoder_name}_{pretrained}\",\n",
    "        'train_fg_patches': len(train_fg_patches),\n",
    "        'train_bg_patches': len(train_bg_patches),\n",
    "        'val_patches': len(val_patches),\n",
    "        'best_val_iou': history['best_val_iou'],\n",
    "        'final_val_dice': history['val_dices'][-1],\n",
    "        'final_train_loss': history['train_losses'][-1],\n",
    "        'final_val_loss': history['val_losses'][-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"RESULTS: IoU={result['best_val_iou']:.4f} | Dice={result['final_val_dice']:.4f}\")\n",
    "    \n",
    "    return result, history\n",
    "\n",
    "def run_full_patch_experiment():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Starting MDSMG Patch-Based Al-Mg-Si-XCT Experiment on {device}\")\n",
    "    \n",
    "    datasets = ['Al-Mg-Si-XCT']\n",
    "    encoders = ['resnet50', 'se_resnext101']\n",
    "    decoders = ['unet', 'deeplabv3', 'deeplabv3plus']\n",
    "    pretrained_options = ['imagenet', 'micronet']\n",
    "    \n",
    "    results = []\n",
    "    histories = {}\n",
    "    experiment_count = 0\n",
    "    \n",
    "    total_experiments = len(datasets) * len(encoders) * len(decoders) * len(pretrained_options)\n",
    "    print(f\"Total experiments to run: {total_experiments}\")\n",
    "    \n",
    "    for dataset_name in datasets:\n",
    "        for encoder in encoders:\n",
    "            for decoder in decoders:\n",
    "                for pretrained in pretrained_options:\n",
    "                    experiment_count += 1\n",
    "                    \n",
    "                    print(f\"Running experiment {experiment_count}/{total_experiments}: {encoder}_{decoder}_{pretrained}\")\n",
    "                    \n",
    "                    result, history = run_patch_experiment(\n",
    "                        dataset_name, encoder, decoder, pretrained, device\n",
    "                    )\n",
    "                    \n",
    "                    if result and history:\n",
    "                        results.append(result)\n",
    "                        histories[result['config'] + '_' + dataset_name] = history\n",
    "                        print(f\"Experiment {experiment_count}/{total_experiments} completed: {result['config']}\")\n",
    "                    else:\n",
    "                        print(f\"Experiment {experiment_count}/{total_experiments} failed or skipped\")\n",
    "    \n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv('patch_almgsi_experiment_results.csv', index=False)\n",
    "        \n",
    "        print(f\"Patch-based Al-Mg-Si-XCT experiment suite completed!\")\n",
    "        print(f\"Total successful experiments: {len(results)}\")\n",
    "        print(f\"Results saved to: patch_almgsi_experiment_results.csv\")\n",
    "        \n",
    "        return results_df, histories\n",
    "    else:\n",
    "        print(\"No successful experiments completed\")\n",
    "        return pd.DataFrame(), {}\n",
    "\n",
    "def analyze_patch_results(results_df):\n",
    "    print(\"PATCH-BASED EXPERIMENT RESULTS ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Total experiments: {len(results_df)}\")\n",
    "    print(f\"Mean IoU across all experiments: {results_df['best_val_iou'].mean():.4f}\")\n",
    "    print(f\"Best overall IoU: {results_df['best_val_iou'].max():.4f}\")\n",
    "    \n",
    "    print(\"\\nPERFORMANCE BY ENCODER:\")\n",
    "    encoder_performance = results_df.groupby('encoder')['best_val_iou'].agg(['mean', 'std', 'max', 'count'])\n",
    "    encoder_performance.columns = ['Mean_IoU', 'Std_IoU', 'Max_IoU', 'Count']\n",
    "    print(encoder_performance.round(4))\n",
    "    \n",
    "    print(\"\\nPERFORMANCE BY DECODER:\")\n",
    "    decoder_performance = results_df.groupby('decoder')['best_val_iou'].agg(['mean', 'std', 'max', 'count'])\n",
    "    decoder_performance.columns = ['Mean_IoU', 'Std_IoU', 'Max_IoU', 'Count']\n",
    "    print(decoder_performance.round(4))\n",
    "    \n",
    "    print(\"\\nPERFORMANCE BY PRETRAINING:\")\n",
    "    pretrained_performance = results_df.groupby('pretrained')['best_val_iou'].agg(['mean', 'std', 'max', 'count'])\n",
    "    pretrained_performance.columns = ['Mean_IoU', 'Std_IoU', 'Max_IoU', 'Count']\n",
    "    print(pretrained_performance.round(4))\n",
    "    \n",
    "    best_idx = results_df['best_val_iou'].idxmax()\n",
    "    best_config = results_df.loc[best_idx]\n",
    "    print(f\"\\nBEST CONFIGURATION:\")\n",
    "    print(f\"Config: {best_config['config']}\")\n",
    "    print(f\"IoU: {best_config['best_val_iou']:.4f}\")\n",
    "    print(f\"Dice: {best_config['final_val_dice']:.4f}\")\n",
    "    print(f\"FG Patches: {best_config['train_fg_patches']}\")\n",
    "    print(f\"BG Patches: {best_config['train_bg_patches']}\")\n",
    "    \n",
    "    return encoder_performance, decoder_performance, pretrained_performance\n",
    "\n",
    "def statistical_analysis_patch(results_df):\n",
    "    print(\"STATISTICAL ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    resnet50_ious = results_df[results_df['encoder'] == 'resnet50']['best_val_iou']\n",
    "    se_resnext_ious = results_df[results_df['encoder'] == 'se_resnext101']['best_val_iou']\n",
    "    \n",
    "    if len(resnet50_ious) > 0 and len(se_resnext_ious) > 0:\n",
    "        t_stat, p_val = stats.ttest_ind(resnet50_ious, se_resnext_ious)\n",
    "        print(f\"Encoder comparison (ResNet50 vs SE-ResNeXt101):\")\n",
    "        print(f\"  t-statistic: {t_stat:.4f}, p-value: {p_val:.4f}\")\n",
    "        if p_val < 0.05:\n",
    "            print(f\"  Significant difference (p < 0.05)\")\n",
    "        else:\n",
    "            print(f\"  No significant difference (p >= 0.05)\")\n",
    "    \n",
    "    imagenet_ious = results_df[results_df['pretrained'] == 'imagenet']['best_val_iou']\n",
    "    micronet_ious = results_df[results_df['pretrained'] == 'micronet']['best_val_iou']\n",
    "    \n",
    "    if len(imagenet_ious) > 0 and len(micronet_ious) > 0:\n",
    "        t_stat, p_val = stats.ttest_ind(imagenet_ious, micronet_ious)\n",
    "        print(f\"\\nPretraining comparison (ImageNet vs MicroNet):\")\n",
    "        print(f\"  t-statistic: {t_stat:.4f}, p-value: {p_val:.4f}\")\n",
    "        if p_val < 0.05:\n",
    "            print(f\"  Significant difference (p < 0.05)\")\n",
    "            if micronet_ious.mean() > imagenet_ious.mean():\n",
    "                print(f\"  -> MicroNet performs significantly better\")\n",
    "            else:\n",
    "                print(f\"  -> ImageNet performs significantly better\")\n",
    "        else:\n",
    "            print(f\"  No significant difference (p >= 0.05)\")\n",
    "    \n",
    "    decoder_groups = [group['best_val_iou'].values for name, group in results_df.groupby('decoder')]\n",
    "    if len(decoder_groups) >= 2 and all(len(group) > 0 for group in decoder_groups):\n",
    "        f_stat, p_val = stats.f_oneway(*decoder_groups)\n",
    "        print(f\"\\nDecoder comparison (ANOVA):\")\n",
    "        print(f\"  F-statistic: {f_stat:.4f}, p-value: {p_val:.4f}\")\n",
    "        if p_val < 0.05:\n",
    "            print(f\"  Significant differences among decoders (p < 0.05)\")\n",
    "        else:\n",
    "            print(f\"  No significant differences among decoders (p >= 0.05)\")\n",
    "\n",
    "def visualize_patch_comparison(results_df, histories):\n",
    "    if len(results_df) == 0:\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Patch-Based Training Results Comparison', fontsize=16)\n",
    "    \n",
    "    # IoU comparison by configuration\n",
    "    configs = results_df['config'].values\n",
    "    ious = results_df['best_val_iou'].values\n",
    "    dices = results_df['final_val_dice'].values\n",
    "    \n",
    "    axes[0, 0].bar(range(len(configs)), ious)\n",
    "    axes[0, 0].set_title('Best IoU by Configuration')\n",
    "    axes[0, 0].set_xlabel('Configuration')\n",
    "    axes[0, 0].set_ylabel('IoU')\n",
    "    axes[0, 0].set_xticks(range(len(configs)))\n",
    "    axes[0, 0].set_xticklabels(configs, rotation=45, ha='right')\n",
    "    \n",
    "    axes[0, 1].bar(range(len(configs)), dices)\n",
    "    axes[0, 1].set_title('Final Dice by Configuration')\n",
    "    axes[0, 1].set_xlabel('Configuration')\n",
    "    axes[0, 1].set_ylabel('Dice')\n",
    "    axes[0, 1].set_xticks(range(len(configs)))\n",
    "    axes[0, 1].set_xticklabels(configs, rotation=45, ha='right')\n",
    "    \n",
    "    # Training curves for best models\n",
    "    best_configs = results_df.nlargest(4, 'best_val_iou')['config'].tolist()\n",
    "    \n",
    "    for i, config in enumerate(best_configs[:2]):\n",
    "        if f\"{config}_Al-Mg-Si-XCT\" in histories:\n",
    "            history = histories[f\"{config}_Al-Mg-Si-XCT\"]\n",
    "            epochs = range(1, len(history['val_ious']) + 1)\n",
    "            axes[1, 0].plot(epochs, history['val_ious'], label=config)\n",
    "    \n",
    "    axes[1, 0].set_title('IoU Learning Curves (Top 2 Models)')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('IoU')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    for i, config in enumerate(best_configs[:2]):\n",
    "        if f\"{config}_Al-Mg-Si-XCT\" in histories:\n",
    "            history = histories[f\"{config}_Al-Mg-Si-XCT\"]\n",
    "            epochs = range(1, len(history['val_dices']) + 1)\n",
    "            axes[1, 1].plot(epochs, history['val_dices'], label=config)\n",
    "    \n",
    "    axes[1, 1].set_title('Dice Learning Curves (Top 2 Models)')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Dice')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('patch_experiment_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def main_patch_experiment():\n",
    "    print(\"MDSMG: Material Data Science Model Garden\")\n",
    "    print(\"Patch-Based Al-Mg-Si-XCT Dataset Experiment\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"Starting patch-based Al-Mg-Si-XCT experiment...\")\n",
    "    patch_results, patch_histories = run_full_patch_experiment()\n",
    "    \n",
    "    if len(patch_results) > 0:\n",
    "        print(\"\\nAnalyzing patch-based results...\")\n",
    "        encoder_perf, decoder_perf, pretrain_perf = analyze_patch_results(patch_results)\n",
    "        \n",
    "        print(\"\\nStatistical analysis...\")\n",
    "        statistical_analysis_patch(patch_results)\n",
    "        \n",
    "        print(\"\\nVisualizing results...\")\n",
    "        visualize_patch_comparison(patch_results, patch_histories)\n",
    "        \n",
    "        print(\"\\nSaving detailed results...\")\n",
    "        encoder_perf.to_csv('patch_encoder_performance.csv')\n",
    "        decoder_perf.to_csv('patch_decoder_performance.csv')\n",
    "        pretrain_perf.to_csv('patch_pretraining_performance.csv')\n",
    "        \n",
    "        print(\"Patch-based analysis completed!\")\n",
    "        print(\"Files saved:\")\n",
    "        print(\"  - patch_almgsi_experiment_results.csv\")\n",
    "        print(\"  - patch_encoder_performance.csv\")\n",
    "        print(\"  - patch_decoder_performance.csv\") \n",
    "        print(\"  - patch_pretraining_performance.csv\")\n",
    "        print(\"  - patch_experiment_comparison.png\")\n",
    "        \n",
    "        return patch_results, patch_histories\n",
    "    else:\n",
    "        print(\"No successful experiments to analyze\")\n",
    "        return None, None\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"GPU Memory available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "    print(\"Starting patch-based MDSMG experiment...\")\n",
    "    patch_results, patch_histories = main_patch_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7aa108-a34d-4388-b2cb-da7c0f0c2a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
